{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfu9giPQ77DL"
   },
   "source": [
    "# Deep Learning Project \\#1\n",
    "\n",
    "## Longer Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvsuoaC8cj5_"
   },
   "source": [
    "### Initial Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VVRwR8OvAHWh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX4LunnPX7d5"
   },
   "source": [
    "Downloading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "91e133612d8c447897c0e9c1634c9aff",
      "0081205de10a45eb95ab0fa8cf5d1b54",
      "a1fd6468adf1485fbd287ffbd8486e72",
      "7586a4d1d847421494ec6c24b7726da5",
      "a078c1f3c09844b68e8147517c5cdd9b",
      "9148add8cc434e6085a7e0534d3040b3",
      "69d639e7861e40a2b3f41448d639c69c",
      "db8b150cbf3f4104834164a15b7b95dd",
      "169e435fd1af40afbb886aa420d0308b",
      "d6c49e0105bd4cba86c7cfdc02bc68b8",
      "8085b344ece049e1a9056e65dd8fc3a3"
     ]
    },
    "id": "3VBZj2vUX7d5",
    "outputId": "26017dfa-d773-4bb9-86de-c90b8b26d883",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainingdata = torchvision.datasets.CIFAR10('./CIFAR-10/',train=True,download=True)\n",
    "testdata = torchvision.datasets.CIFAR10('./CIFAR-10/',train=False,download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za6-pu8lX7d5"
   },
   "source": [
    "Mean and Std calculation for normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8kZxJ_FX7d6",
    "outputId": "65b2564b-1665-488d-e676-7f130048c6b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49139968 0.48215841 0.44653091]\n",
      "[0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "tr_mean = trainingdata.data.mean(axis = (0,1,2)) / 255\n",
    "tr_std = trainingdata.data.std(axis = (0,1,2))  / 255\n",
    "print(tr_mean)\n",
    "print(tr_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43O3-oLlur-U"
   },
   "source": [
    "Data Augmentation (Random Transforms and Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OfkgRIlwX7d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_train = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomCrop(size=[32,32], padding=4),\n",
    "        torchvision.transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        torchvision.transforms.RandomRotation(10),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(tr_mean, tr_std)\n",
    "    ])\n",
    "\n",
    "tr_test = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                          torchvision.transforms.Normalize(tr_mean, tr_std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmjvpbYVX7d8"
   },
   "source": [
    "Download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DqjmlixNCMlZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainingdata = torchvision.datasets.CIFAR10('./CIFAR-10/',train=True,download=False,transform=tr_train)\n",
    "testdata = torchvision.datasets.CIFAR10('./CIFAR-10/',train=False,download=False,transform=tr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8jilneICxoQ"
   },
   "source": [
    "Check how many Training and Test Data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIyuKhE-X7d9",
    "outputId": "d15cc8e0-9b20-41d3-c067-e090e621a1e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainingdata))\n",
    "print(len(testdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq_r5NZ6C4qp"
   },
   "source": [
    "Printing size of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESuk-dMuDI50",
    "outputId": "085dff51-609c-4f7d-fb25-53d96010c1e6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 2\n"
     ]
    }
   ],
   "source": [
    "image, label = trainingdata[13]\n",
    "print(image.size(),label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xm_XdCDE9bY"
   },
   "source": [
    "## Generic ResNet Model:\n",
    "\n",
    "### We defined our model in a parametric way\n",
    "\n",
    "So here we can set up our parameters later\n",
    "\n",
    "N: Number of residual layers, 3-4-5  \n",
    "C_1: # channels in the first layer  \n",
    "num_blocks: Nx1 vector, num_blocks[i] is number of residual blocks in layer i  \n",
    "conv_kernel_size: Nx1 vector, kernel size for conv layers in each residual layer  \n",
    "skip_kernel_size: Nx1 vector, kernel size for skip connections between each residual block  \n",
    "avg_pool_size: Kernel size for the average pooling at last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fJy4Ow0WJ6Te",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, kernel_size, skip_kernel_size, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size, stride=stride, padding=int((kernel_size-1)/2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size,\n",
    "                               stride=1, padding=int((kernel_size-1)/2), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          skip_kernel_size, stride=stride, padding=int((skip_kernel_size-1)/2), bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, N, num_blocks, C_1, conv_kernel_size, skip_kernel_size, avg_pool_size, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = C_1\n",
    "        self.avg_pool_size = avg_pool_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, C_1, conv_kernel_size[0],\n",
    "                               stride=1, padding=int((conv_kernel_size[0]-1)/2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(C_1)\n",
    "\n",
    "        res_layers = []\n",
    "        \n",
    "        for i in range(N):\n",
    "          if i == 0:\n",
    "            s = 1\n",
    "          else:\n",
    "            s = 2\n",
    "          res_layers.append(self._make_layer(block, (2**i)*C_1, num_blocks[i], conv_kernel_size[i], skip_kernel_size[i], stride=s))\n",
    "\n",
    "        self.layer = nn.Sequential(*res_layers)\n",
    "\n",
    "        ps = 2**(np.ceil(np.log2(avg_pool_size)))\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(int(C_1*32*32/(2**(N-1))/(ps**2)), num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, conv_kernel_size, skip_kernel_size, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, conv_kernel_size, skip_kernel_size, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer(out)\n",
    "        out = F.avg_pool2d(out, self.avg_pool_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def project1_model(N, num_blocks, C_1, conv_kernel_size, skip_kernel_size, avg_pool_size):\n",
    "    return ResNet(BasicBlock, N, num_blocks, C_1, conv_kernel_size, skip_kernel_size, avg_pool_size, num_classes=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vpZlJ09bEWb"
   },
   "source": [
    "### Parameters From Previous Runs in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JIef9lQHbKEn"
   },
   "outputs": [],
   "source": [
    "Grid_Search = ((2,1,256), #1\n",
    "              (2,2,172), #2\n",
    "              (2,3,140), #3\n",
    "              (2,4,120), #4\n",
    "              (2,5,106), #5\n",
    "              (2,6,96),  #6\n",
    "              (2,7,90),  #7\n",
    "              (3,1,128), #8\n",
    "              (3,2,85),  #9\n",
    "              (3,3,68),  #10\n",
    "              (3,4,58),  #11\n",
    "              (3,5,52),  #12\n",
    "              (4,1,64),  #13\n",
    "              (4,2,42),  #14\n",
    "              (4,3,32),  #15\n",
    "              (4,4,29),  #16\n",
    "              (4,5,26),  #17\n",
    "              (5,1,32),  #18\n",
    "              (5,2,21),  #19\n",
    "              (5,3,17),  #20\n",
    "              (5,4,14),  #21\n",
    "              (5,5,13))  #22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7GG-X2_bMVw"
   },
   "source": [
    "Our top picks are: \n",
    "\n",
    "1.   N=3 B=2 C=85 #9\n",
    "2.   N=3 B=3 C=68 #10\n",
    "3.   N=2 B=3 C=140 #3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68BcmH5QaUy3"
   },
   "source": [
    "### Function to call the model is defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1vLhOfH8aUUZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def neural_net(N,C1,num_blocks,conv_kernel_size,avg_pool_size,skip_kernel_size):\n",
    "    net = project1_model(N,\n",
    "                    num_blocks, \n",
    "                    C1, \n",
    "                    conv_kernel_size,\n",
    "                    skip_kernel_size,\n",
    "                    avg_pool_size).cuda()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FXrsqljcau7"
   },
   "source": [
    "## TRAINING (EDITS START FROM HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AXqzL6uczH_"
   },
   "source": [
    "Setting up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s0b82L9650OV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ind = 8\n",
    "N,B,C = Grid_Search[Ind]\n",
    "\n",
    "num_blocks = [B]*N\n",
    "skip_kernel_size = [1]*N\n",
    "conv_kernel_size = [3]*N\n",
    "avg_pool_size = int(32/(2**(N-1)))\n",
    "net = neural_net(N,C,num_blocks,conv_kernel_size,avg_pool_size,skip_kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfu_9eScc2Fo"
   },
   "source": [
    "Setting up optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GpNJ2kjTdB2r"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "step_size = 10\n",
    "gamma = 0.90\n",
    "Loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0fg3EoedC3y"
   },
   "source": [
    "We set up our data loaders.  \n",
    "We can try different Batch Sizes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NGt6-QvFdDLJ"
   },
   "outputs": [],
   "source": [
    "N_batch = 64\n",
    "\n",
    "trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=N_batch,shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=N_batch,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_lMMzzhX7eA"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FehBnwmzC-WN",
    "outputId": "42d2b100-d74a-440c-968b-f22234300b4e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        --                        --\n",
       "├─Conv2d: 1-1                                 [1, 85, 32, 32]           2,295\n",
       "├─BatchNorm2d: 1-2                            [1, 85, 32, 32]           170\n",
       "├─Sequential: 1-3                             [1, 340, 8, 8]            --\n",
       "│    └─Sequential: 2-1                        [1, 85, 32, 32]           --\n",
       "│    │    └─BasicBlock: 3-1                   [1, 85, 32, 32]           130,390\n",
       "│    │    └─BasicBlock: 3-2                   [1, 85, 32, 32]           130,390\n",
       "│    └─Sequential: 2-2                        [1, 170, 16, 16]          --\n",
       "│    │    └─BasicBlock: 3-3                   [1, 170, 16, 16]          405,620\n",
       "│    │    └─BasicBlock: 3-4                   [1, 170, 16, 16]          520,880\n",
       "│    └─Sequential: 2-3                        [1, 340, 8, 8]            --\n",
       "│    │    └─BasicBlock: 3-5                   [1, 340, 8, 8]            1,620,440\n",
       "│    │    └─BasicBlock: 3-6                   [1, 340, 8, 8]            2,082,160\n",
       "├─Linear: 1-4                                 [1, 10]                   3,410\n",
       "===============================================================================================\n",
       "Total params: 4,895,755\n",
       "Trainable params: 4,895,755\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 742.20\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 12.19\n",
       "Params size (MB): 19.58\n",
       "Estimated Total Size (MB): 31.78\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the model\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "summary(net,(1,3,32,32))\n",
    "\n",
    "\n",
    "# TRY THIS IF ABOVE GIVES ERROR\n",
    "# from torchsummary import summary\n",
    "# from torchvision import models\n",
    "# summary(net,(3,32,32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs9-dDLsKFrc"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "remZVoddX7eB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTrain Loss: 1.78522 \tTrain Acc: 0.3337 \tTest Loss: 1.52942 \tTest Acc: 0.4400 \tTime: 72.2150\n",
      "Epoch: 1 \tTrain Loss: 1.33652 \tTrain Acc: 0.5177 \tTest Loss: 1.29147 \tTest Acc: 0.5469 \tTime: 68.0801\n",
      "Epoch: 2 \tTrain Loss: 1.09655 \tTrain Acc: 0.6111 \tTest Loss: 1.04110 \tTest Acc: 0.6321 \tTime: 68.2179\n",
      "Epoch: 3 \tTrain Loss: 0.95265 \tTrain Acc: 0.6637 \tTest Loss: 0.92246 \tTest Acc: 0.6808 \tTime: 68.1530\n",
      "Epoch: 4 \tTrain Loss: 0.83932 \tTrain Acc: 0.7058 \tTest Loss: 0.86679 \tTest Acc: 0.7028 \tTime: 68.3162\n",
      "Epoch: 5 \tTrain Loss: 0.74687 \tTrain Acc: 0.7385 \tTest Loss: 0.74028 \tTest Acc: 0.7431 \tTime: 68.1919\n",
      "Epoch: 6 \tTrain Loss: 0.67665 \tTrain Acc: 0.7642 \tTest Loss: 0.72721 \tTest Acc: 0.7428 \tTime: 68.1309\n",
      "Epoch: 7 \tTrain Loss: 0.61589 \tTrain Acc: 0.7861 \tTest Loss: 0.61843 \tTest Acc: 0.7876 \tTime: 67.7611\n",
      "Epoch: 8 \tTrain Loss: 0.57880 \tTrain Acc: 0.7992 \tTest Loss: 0.59258 \tTest Acc: 0.7931 \tTime: 68.1160\n",
      "Epoch: 9 \tTrain Loss: 0.53660 \tTrain Acc: 0.8132 \tTest Loss: 0.53408 \tTest Acc: 0.8163 \tTime: 67.9805\n",
      "Epoch: 10 \tTrain Loss: 0.49103 \tTrain Acc: 0.8306 \tTest Loss: 0.51726 \tTest Acc: 0.8261 \tTime: 68.1685\n",
      "Epoch: 11 \tTrain Loss: 0.46174 \tTrain Acc: 0.8407 \tTest Loss: 0.46491 \tTest Acc: 0.8436 \tTime: 68.2050\n",
      "Epoch: 12 \tTrain Loss: 0.43608 \tTrain Acc: 0.8494 \tTest Loss: 0.49703 \tTest Acc: 0.8323 \tTime: 68.1090\n",
      "Epoch: 13 \tTrain Loss: 0.41023 \tTrain Acc: 0.8578 \tTest Loss: 0.45904 \tTest Acc: 0.8424 \tTime: 68.3065\n",
      "Epoch: 14 \tTrain Loss: 0.39123 \tTrain Acc: 0.8617 \tTest Loss: 0.43817 \tTest Acc: 0.8482 \tTime: 68.1382\n",
      "Epoch: 15 \tTrain Loss: 0.37755 \tTrain Acc: 0.8680 \tTest Loss: 0.42229 \tTest Acc: 0.8544 \tTime: 68.3101\n",
      "Epoch: 16 \tTrain Loss: 0.35854 \tTrain Acc: 0.8726 \tTest Loss: 0.43182 \tTest Acc: 0.8545 \tTime: 68.2642\n",
      "Epoch: 17 \tTrain Loss: 0.34142 \tTrain Acc: 0.8803 \tTest Loss: 0.43376 \tTest Acc: 0.8491 \tTime: 68.3179\n",
      "Epoch: 18 \tTrain Loss: 0.32736 \tTrain Acc: 0.8867 \tTest Loss: 0.39961 \tTest Acc: 0.8664 \tTime: 68.2509\n",
      "Epoch: 19 \tTrain Loss: 0.31595 \tTrain Acc: 0.8898 \tTest Loss: 0.40508 \tTest Acc: 0.8689 \tTime: 68.2803\n",
      "Epoch: 20 \tTrain Loss: 0.29119 \tTrain Acc: 0.8965 \tTest Loss: 0.37812 \tTest Acc: 0.8729 \tTime: 68.2524\n",
      "Epoch: 21 \tTrain Loss: 0.27798 \tTrain Acc: 0.9030 \tTest Loss: 0.39836 \tTest Acc: 0.8688 \tTime: 68.3590\n",
      "Epoch: 22 \tTrain Loss: 0.27104 \tTrain Acc: 0.9069 \tTest Loss: 0.35257 \tTest Acc: 0.8847 \tTime: 68.3733\n",
      "Epoch: 23 \tTrain Loss: 0.25862 \tTrain Acc: 0.9082 \tTest Loss: 0.37197 \tTest Acc: 0.8776 \tTime: 68.3408\n",
      "Epoch: 24 \tTrain Loss: 0.24971 \tTrain Acc: 0.9118 \tTest Loss: 0.37026 \tTest Acc: 0.8765 \tTime: 68.0851\n",
      "Epoch: 25 \tTrain Loss: 0.24137 \tTrain Acc: 0.9142 \tTest Loss: 0.36620 \tTest Acc: 0.8791 \tTime: 68.2120\n",
      "Epoch: 26 \tTrain Loss: 0.23480 \tTrain Acc: 0.9174 \tTest Loss: 0.37259 \tTest Acc: 0.8773 \tTime: 68.4271\n",
      "Epoch: 27 \tTrain Loss: 0.22415 \tTrain Acc: 0.9204 \tTest Loss: 0.35481 \tTest Acc: 0.8846 \tTime: 68.1399\n",
      "Epoch: 28 \tTrain Loss: 0.21780 \tTrain Acc: 0.9228 \tTest Loss: 0.37386 \tTest Acc: 0.8811 \tTime: 68.2066\n",
      "Epoch: 29 \tTrain Loss: 0.21771 \tTrain Acc: 0.9237 \tTest Loss: 0.37849 \tTest Acc: 0.8788 \tTime: 68.2374\n",
      "Epoch: 30 \tTrain Loss: 0.19863 \tTrain Acc: 0.9307 \tTest Loss: 0.34682 \tTest Acc: 0.8880 \tTime: 68.3715\n",
      "Epoch: 31 \tTrain Loss: 0.19171 \tTrain Acc: 0.9326 \tTest Loss: 0.37873 \tTest Acc: 0.8829 \tTime: 68.3052\n",
      "Epoch: 32 \tTrain Loss: 0.18381 \tTrain Acc: 0.9359 \tTest Loss: 0.36639 \tTest Acc: 0.8853 \tTime: 68.1712\n",
      "Epoch: 33 \tTrain Loss: 0.18317 \tTrain Acc: 0.9349 \tTest Loss: 0.34818 \tTest Acc: 0.8882 \tTime: 68.2148\n",
      "Epoch: 34 \tTrain Loss: 0.17522 \tTrain Acc: 0.9391 \tTest Loss: 0.35970 \tTest Acc: 0.8890 \tTime: 68.2469\n",
      "Epoch: 35 \tTrain Loss: 0.17078 \tTrain Acc: 0.9414 \tTest Loss: 0.34984 \tTest Acc: 0.8935 \tTime: 68.2931\n",
      "Epoch: 36 \tTrain Loss: 0.16552 \tTrain Acc: 0.9408 \tTest Loss: 0.36431 \tTest Acc: 0.8909 \tTime: 68.1740\n",
      "Epoch: 37 \tTrain Loss: 0.16076 \tTrain Acc: 0.9431 \tTest Loss: 0.36587 \tTest Acc: 0.8901 \tTime: 68.3250\n",
      "Epoch: 38 \tTrain Loss: 0.15520 \tTrain Acc: 0.9463 \tTest Loss: 0.35434 \tTest Acc: 0.8962 \tTime: 68.2468\n",
      "Epoch: 39 \tTrain Loss: 0.15296 \tTrain Acc: 0.9459 \tTest Loss: 0.35699 \tTest Acc: 0.8892 \tTime: 68.3948\n",
      "Epoch: 40 \tTrain Loss: 0.14015 \tTrain Acc: 0.9508 \tTest Loss: 0.34275 \tTest Acc: 0.8987 \tTime: 68.1837\n",
      "Epoch: 41 \tTrain Loss: 0.13921 \tTrain Acc: 0.9520 \tTest Loss: 0.35177 \tTest Acc: 0.8961 \tTime: 68.3537\n",
      "Epoch: 42 \tTrain Loss: 0.13006 \tTrain Acc: 0.9547 \tTest Loss: 0.35990 \tTest Acc: 0.8958 \tTime: 68.2875\n",
      "Epoch: 43 \tTrain Loss: 0.13600 \tTrain Acc: 0.9517 \tTest Loss: 0.32785 \tTest Acc: 0.9001 \tTime: 68.3246\n",
      "Epoch: 44 \tTrain Loss: 0.12801 \tTrain Acc: 0.9548 \tTest Loss: 0.36806 \tTest Acc: 0.8934 \tTime: 68.1258\n",
      "Epoch: 45 \tTrain Loss: 0.12496 \tTrain Acc: 0.9567 \tTest Loss: 0.35402 \tTest Acc: 0.9006 \tTime: 68.2271\n",
      "Epoch: 46 \tTrain Loss: 0.12450 \tTrain Acc: 0.9556 \tTest Loss: 0.36891 \tTest Acc: 0.8917 \tTime: 68.3290\n",
      "Epoch: 47 \tTrain Loss: 0.12268 \tTrain Acc: 0.9573 \tTest Loss: 0.36212 \tTest Acc: 0.8969 \tTime: 68.2295\n",
      "Epoch: 48 \tTrain Loss: 0.12516 \tTrain Acc: 0.9571 \tTest Loss: 0.38459 \tTest Acc: 0.8903 \tTime: 68.3911\n",
      "Epoch: 49 \tTrain Loss: 0.11756 \tTrain Acc: 0.9591 \tTest Loss: 0.35863 \tTest Acc: 0.9001 \tTime: 68.2926\n",
      "Epoch: 50 \tTrain Loss: 0.10342 \tTrain Acc: 0.9637 \tTest Loss: 0.38669 \tTest Acc: 0.8985 \tTime: 68.1972\n",
      "Epoch: 51 \tTrain Loss: 0.10476 \tTrain Acc: 0.9634 \tTest Loss: 0.33840 \tTest Acc: 0.9010 \tTime: 68.1968\n",
      "Epoch: 52 \tTrain Loss: 0.10413 \tTrain Acc: 0.9639 \tTest Loss: 0.36801 \tTest Acc: 0.8992 \tTime: 68.2411\n",
      "Epoch: 53 \tTrain Loss: 0.10000 \tTrain Acc: 0.9643 \tTest Loss: 0.37093 \tTest Acc: 0.8953 \tTime: 68.3245\n",
      "Epoch: 54 \tTrain Loss: 0.09883 \tTrain Acc: 0.9667 \tTest Loss: 0.34685 \tTest Acc: 0.9040 \tTime: 68.3406\n",
      "Epoch: 55 \tTrain Loss: 0.09631 \tTrain Acc: 0.9661 \tTest Loss: 0.35821 \tTest Acc: 0.9022 \tTime: 68.2913\n",
      "Epoch: 56 \tTrain Loss: 0.09507 \tTrain Acc: 0.9669 \tTest Loss: 0.36803 \tTest Acc: 0.9026 \tTime: 68.3645\n",
      "Epoch: 57 \tTrain Loss: 0.09150 \tTrain Acc: 0.9678 \tTest Loss: 0.34715 \tTest Acc: 0.9056 \tTime: 68.3340\n",
      "Epoch: 58 \tTrain Loss: 0.09412 \tTrain Acc: 0.9678 \tTest Loss: 0.36542 \tTest Acc: 0.9018 \tTime: 68.3685\n",
      "Epoch: 59 \tTrain Loss: 0.09363 \tTrain Acc: 0.9677 \tTest Loss: 0.36066 \tTest Acc: 0.9000 \tTime: 68.2241\n",
      "Epoch: 60 \tTrain Loss: 0.08275 \tTrain Acc: 0.9716 \tTest Loss: 0.37319 \tTest Acc: 0.9024 \tTime: 68.2840\n",
      "Epoch: 61 \tTrain Loss: 0.08250 \tTrain Acc: 0.9704 \tTest Loss: 0.37436 \tTest Acc: 0.8996 \tTime: 68.2969\n",
      "Epoch: 62 \tTrain Loss: 0.08121 \tTrain Acc: 0.9717 \tTest Loss: 0.38456 \tTest Acc: 0.9023 \tTime: 68.3303\n",
      "Epoch: 63 \tTrain Loss: 0.07991 \tTrain Acc: 0.9720 \tTest Loss: 0.36677 \tTest Acc: 0.9049 \tTime: 68.2212\n",
      "Epoch: 64 \tTrain Loss: 0.08094 \tTrain Acc: 0.9721 \tTest Loss: 0.38752 \tTest Acc: 0.8982 \tTime: 68.2709\n",
      "Epoch: 65 \tTrain Loss: 0.07440 \tTrain Acc: 0.9738 \tTest Loss: 0.37767 \tTest Acc: 0.9009 \tTime: 68.2271\n",
      "Epoch: 66 \tTrain Loss: 0.07601 \tTrain Acc: 0.9739 \tTest Loss: 0.38613 \tTest Acc: 0.9029 \tTime: 68.3266\n",
      "Epoch: 67 \tTrain Loss: 0.07608 \tTrain Acc: 0.9736 \tTest Loss: 0.37690 \tTest Acc: 0.9055 \tTime: 68.2046\n",
      "Epoch: 68 \tTrain Loss: 0.07328 \tTrain Acc: 0.9749 \tTest Loss: 0.37320 \tTest Acc: 0.9019 \tTime: 68.0620\n",
      "Epoch: 69 \tTrain Loss: 0.07139 \tTrain Acc: 0.9747 \tTest Loss: 0.38351 \tTest Acc: 0.9033 \tTime: 68.0900\n",
      "Epoch: 70 \tTrain Loss: 0.06882 \tTrain Acc: 0.9766 \tTest Loss: 0.38044 \tTest Acc: 0.9068 \tTime: 68.2605\n",
      "Epoch: 71 \tTrain Loss: 0.06662 \tTrain Acc: 0.9770 \tTest Loss: 0.37668 \tTest Acc: 0.9021 \tTime: 68.0544\n",
      "Epoch: 72 \tTrain Loss: 0.06389 \tTrain Acc: 0.9779 \tTest Loss: 0.38868 \tTest Acc: 0.9058 \tTime: 68.1657\n",
      "Epoch: 73 \tTrain Loss: 0.06430 \tTrain Acc: 0.9780 \tTest Loss: 0.36814 \tTest Acc: 0.9087 \tTime: 68.2392\n",
      "Epoch: 74 \tTrain Loss: 0.06187 \tTrain Acc: 0.9783 \tTest Loss: 0.38479 \tTest Acc: 0.9035 \tTime: 68.2953\n",
      "Epoch: 75 \tTrain Loss: 0.06729 \tTrain Acc: 0.9761 \tTest Loss: 0.37847 \tTest Acc: 0.9081 \tTime: 68.2166\n",
      "Epoch: 76 \tTrain Loss: 0.06289 \tTrain Acc: 0.9789 \tTest Loss: 0.37681 \tTest Acc: 0.9098 \tTime: 68.4052\n",
      "Epoch: 77 \tTrain Loss: 0.05966 \tTrain Acc: 0.9792 \tTest Loss: 0.40202 \tTest Acc: 0.9049 \tTime: 68.2233\n",
      "Epoch: 78 \tTrain Loss: 0.06145 \tTrain Acc: 0.9790 \tTest Loss: 0.37746 \tTest Acc: 0.9076 \tTime: 68.2318\n",
      "Epoch: 79 \tTrain Loss: 0.05758 \tTrain Acc: 0.9799 \tTest Loss: 0.37810 \tTest Acc: 0.9073 \tTime: 68.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 \tTrain Loss: 0.05596 \tTrain Acc: 0.9813 \tTest Loss: 0.37625 \tTest Acc: 0.9069 \tTime: 68.3496\n",
      "Epoch: 81 \tTrain Loss: 0.05764 \tTrain Acc: 0.9804 \tTest Loss: 0.38576 \tTest Acc: 0.9051 \tTime: 68.1995\n",
      "Epoch: 82 \tTrain Loss: 0.05330 \tTrain Acc: 0.9819 \tTest Loss: 0.40400 \tTest Acc: 0.9064 \tTime: 68.2944\n",
      "Epoch: 83 \tTrain Loss: 0.05291 \tTrain Acc: 0.9815 \tTest Loss: 0.36946 \tTest Acc: 0.9075 \tTime: 68.3051\n",
      "Epoch: 84 \tTrain Loss: 0.05324 \tTrain Acc: 0.9818 \tTest Loss: 0.40175 \tTest Acc: 0.9070 \tTime: 68.3204\n",
      "Epoch: 85 \tTrain Loss: 0.05325 \tTrain Acc: 0.9820 \tTest Loss: 0.38824 \tTest Acc: 0.9083 \tTime: 68.2323\n",
      "Epoch: 86 \tTrain Loss: 0.05082 \tTrain Acc: 0.9824 \tTest Loss: 0.37504 \tTest Acc: 0.9087 \tTime: 68.2561\n",
      "Epoch: 87 \tTrain Loss: 0.05081 \tTrain Acc: 0.9822 \tTest Loss: 0.38738 \tTest Acc: 0.9123 \tTime: 68.2191\n",
      "Epoch: 88 \tTrain Loss: 0.05043 \tTrain Acc: 0.9830 \tTest Loss: 0.36981 \tTest Acc: 0.9097 \tTime: 68.1418\n",
      "Epoch: 89 \tTrain Loss: 0.05008 \tTrain Acc: 0.9825 \tTest Loss: 0.41778 \tTest Acc: 0.9023 \tTime: 68.2936\n",
      "Epoch: 90 \tTrain Loss: 0.04449 \tTrain Acc: 0.9849 \tTest Loss: 0.39346 \tTest Acc: 0.9107 \tTime: 68.9615\n",
      "Epoch: 91 \tTrain Loss: 0.04685 \tTrain Acc: 0.9841 \tTest Loss: 0.39272 \tTest Acc: 0.9077 \tTime: 68.2969\n",
      "Epoch: 92 \tTrain Loss: 0.04658 \tTrain Acc: 0.9841 \tTest Loss: 0.37915 \tTest Acc: 0.9110 \tTime: 68.3094\n",
      "Epoch: 93 \tTrain Loss: 0.04685 \tTrain Acc: 0.9841 \tTest Loss: 0.38633 \tTest Acc: 0.9061 \tTime: 68.3038\n",
      "Epoch: 94 \tTrain Loss: 0.04426 \tTrain Acc: 0.9840 \tTest Loss: 0.39586 \tTest Acc: 0.9091 \tTime: 68.1651\n",
      "Epoch: 95 \tTrain Loss: 0.04211 \tTrain Acc: 0.9858 \tTest Loss: 0.40639 \tTest Acc: 0.9110 \tTime: 68.2977\n",
      "Epoch: 96 \tTrain Loss: 0.04505 \tTrain Acc: 0.9848 \tTest Loss: 0.40174 \tTest Acc: 0.9087 \tTime: 68.3898\n",
      "Epoch: 97 \tTrain Loss: 0.04419 \tTrain Acc: 0.9848 \tTest Loss: 0.41049 \tTest Acc: 0.9054 \tTime: 68.3195\n",
      "Epoch: 98 \tTrain Loss: 0.04275 \tTrain Acc: 0.9854 \tTest Loss: 0.40256 \tTest Acc: 0.9036 \tTime: 68.4056\n",
      "Epoch: 99 \tTrain Loss: 0.04082 \tTrain Acc: 0.9855 \tTest Loss: 0.41064 \tTest Acc: 0.9061 \tTime: 68.0352\n",
      "Epoch: 100 \tTrain Loss: 0.03891 \tTrain Acc: 0.9868 \tTest Loss: 0.41446 \tTest Acc: 0.9042 \tTime: 68.2728\n",
      "Epoch: 101 \tTrain Loss: 0.03783 \tTrain Acc: 0.9871 \tTest Loss: 0.41123 \tTest Acc: 0.9112 \tTime: 68.3697\n",
      "Epoch: 102 \tTrain Loss: 0.04291 \tTrain Acc: 0.9852 \tTest Loss: 0.40426 \tTest Acc: 0.9088 \tTime: 68.2579\n",
      "Epoch: 103 \tTrain Loss: 0.03890 \tTrain Acc: 0.9871 \tTest Loss: 0.40105 \tTest Acc: 0.9096 \tTime: 68.3260\n",
      "Epoch: 104 \tTrain Loss: 0.03995 \tTrain Acc: 0.9863 \tTest Loss: 0.40305 \tTest Acc: 0.9092 \tTime: 68.2151\n",
      "Epoch: 105 \tTrain Loss: 0.03775 \tTrain Acc: 0.9873 \tTest Loss: 0.40140 \tTest Acc: 0.9105 \tTime: 68.3491\n",
      "Epoch: 106 \tTrain Loss: 0.03830 \tTrain Acc: 0.9867 \tTest Loss: 0.36658 \tTest Acc: 0.9157 \tTime: 68.3466\n",
      "Epoch: 107 \tTrain Loss: 0.03870 \tTrain Acc: 0.9868 \tTest Loss: 0.38172 \tTest Acc: 0.9132 \tTime: 68.2605\n",
      "Epoch: 108 \tTrain Loss: 0.03757 \tTrain Acc: 0.9872 \tTest Loss: 0.38985 \tTest Acc: 0.9125 \tTime: 68.2411\n",
      "Epoch: 109 \tTrain Loss: 0.03671 \tTrain Acc: 0.9877 \tTest Loss: 0.42434 \tTest Acc: 0.9062 \tTime: 68.3780\n",
      "Epoch: 110 \tTrain Loss: 0.03358 \tTrain Acc: 0.9888 \tTest Loss: 0.40678 \tTest Acc: 0.9077 \tTime: 68.2857\n",
      "Epoch: 111 \tTrain Loss: 0.03424 \tTrain Acc: 0.9885 \tTest Loss: 0.39762 \tTest Acc: 0.9116 \tTime: 68.2169\n",
      "Epoch: 112 \tTrain Loss: 0.03214 \tTrain Acc: 0.9896 \tTest Loss: 0.41733 \tTest Acc: 0.9067 \tTime: 68.2337\n",
      "Epoch: 113 \tTrain Loss: 0.03610 \tTrain Acc: 0.9881 \tTest Loss: 0.38104 \tTest Acc: 0.9133 \tTime: 68.3033\n",
      "Epoch: 114 \tTrain Loss: 0.03459 \tTrain Acc: 0.9883 \tTest Loss: 0.37644 \tTest Acc: 0.9116 \tTime: 68.3159\n",
      "Epoch: 115 \tTrain Loss: 0.03157 \tTrain Acc: 0.9896 \tTest Loss: 0.39812 \tTest Acc: 0.9144 \tTime: 68.2551\n",
      "Epoch: 116 \tTrain Loss: 0.03320 \tTrain Acc: 0.9887 \tTest Loss: 0.38299 \tTest Acc: 0.9150 \tTime: 68.2358\n",
      "Epoch: 117 \tTrain Loss: 0.03009 \tTrain Acc: 0.9899 \tTest Loss: 0.37748 \tTest Acc: 0.9165 \tTime: 68.2423\n",
      "Epoch: 118 \tTrain Loss: 0.03316 \tTrain Acc: 0.9888 \tTest Loss: 0.41195 \tTest Acc: 0.9090 \tTime: 68.2533\n",
      "Epoch: 119 \tTrain Loss: 0.03286 \tTrain Acc: 0.9887 \tTest Loss: 0.37591 \tTest Acc: 0.9162 \tTime: 68.2775\n",
      "Epoch: 120 \tTrain Loss: 0.02984 \tTrain Acc: 0.9899 \tTest Loss: 0.38474 \tTest Acc: 0.9159 \tTime: 68.2175\n",
      "Epoch: 121 \tTrain Loss: 0.02937 \tTrain Acc: 0.9899 \tTest Loss: 0.37071 \tTest Acc: 0.9200 \tTime: 68.1395\n",
      "Epoch: 122 \tTrain Loss: 0.02925 \tTrain Acc: 0.9901 \tTest Loss: 0.38050 \tTest Acc: 0.9172 \tTime: 68.1640\n",
      "Epoch: 123 \tTrain Loss: 0.02797 \tTrain Acc: 0.9905 \tTest Loss: 0.39004 \tTest Acc: 0.9128 \tTime: 68.1893\n",
      "Epoch: 124 \tTrain Loss: 0.02910 \tTrain Acc: 0.9906 \tTest Loss: 0.39207 \tTest Acc: 0.9123 \tTime: 68.2911\n",
      "Epoch: 125 \tTrain Loss: 0.03048 \tTrain Acc: 0.9893 \tTest Loss: 0.38945 \tTest Acc: 0.9107 \tTime: 68.2455\n",
      "Epoch: 126 \tTrain Loss: 0.02510 \tTrain Acc: 0.9919 \tTest Loss: 0.42167 \tTest Acc: 0.9109 \tTime: 68.1119\n",
      "Epoch: 127 \tTrain Loss: 0.02984 \tTrain Acc: 0.9895 \tTest Loss: 0.39863 \tTest Acc: 0.9155 \tTime: 68.0908\n",
      "Epoch: 128 \tTrain Loss: 0.02732 \tTrain Acc: 0.9904 \tTest Loss: 0.40947 \tTest Acc: 0.9135 \tTime: 68.2257\n",
      "Epoch: 129 \tTrain Loss: 0.02810 \tTrain Acc: 0.9906 \tTest Loss: 0.39176 \tTest Acc: 0.9147 \tTime: 68.1731\n",
      "Epoch: 130 \tTrain Loss: 0.02747 \tTrain Acc: 0.9908 \tTest Loss: 0.39690 \tTest Acc: 0.9134 \tTime: 68.0464\n",
      "Epoch: 131 \tTrain Loss: 0.02485 \tTrain Acc: 0.9914 \tTest Loss: 0.40523 \tTest Acc: 0.9164 \tTime: 68.2309\n",
      "Epoch: 132 \tTrain Loss: 0.02625 \tTrain Acc: 0.9913 \tTest Loss: 0.39915 \tTest Acc: 0.9139 \tTime: 68.0316\n",
      "Epoch: 133 \tTrain Loss: 0.02589 \tTrain Acc: 0.9911 \tTest Loss: 0.38550 \tTest Acc: 0.9171 \tTime: 68.0766\n",
      "Epoch: 134 \tTrain Loss: 0.02465 \tTrain Acc: 0.9914 \tTest Loss: 0.41940 \tTest Acc: 0.9146 \tTime: 68.0611\n",
      "Epoch: 135 \tTrain Loss: 0.02436 \tTrain Acc: 0.9919 \tTest Loss: 0.40943 \tTest Acc: 0.9160 \tTime: 68.2026\n",
      "Epoch: 136 \tTrain Loss: 0.02516 \tTrain Acc: 0.9912 \tTest Loss: 0.40463 \tTest Acc: 0.9128 \tTime: 68.1246\n",
      "Epoch: 137 \tTrain Loss: 0.02446 \tTrain Acc: 0.9917 \tTest Loss: 0.40565 \tTest Acc: 0.9161 \tTime: 68.1525\n",
      "Epoch: 138 \tTrain Loss: 0.02355 \tTrain Acc: 0.9917 \tTest Loss: 0.39842 \tTest Acc: 0.9179 \tTime: 68.1349\n",
      "Epoch: 139 \tTrain Loss: 0.02331 \tTrain Acc: 0.9921 \tTest Loss: 0.40610 \tTest Acc: 0.9167 \tTime: 68.1015\n",
      "Epoch: 140 \tTrain Loss: 0.02196 \tTrain Acc: 0.9928 \tTest Loss: 0.40100 \tTest Acc: 0.9177 \tTime: 68.3262\n",
      "Epoch: 141 \tTrain Loss: 0.02367 \tTrain Acc: 0.9923 \tTest Loss: 0.40576 \tTest Acc: 0.9179 \tTime: 68.2545\n",
      "Epoch: 142 \tTrain Loss: 0.02168 \tTrain Acc: 0.9927 \tTest Loss: 0.39607 \tTest Acc: 0.9184 \tTime: 68.2598\n",
      "Epoch: 143 \tTrain Loss: 0.02065 \tTrain Acc: 0.9924 \tTest Loss: 0.41495 \tTest Acc: 0.9151 \tTime: 68.1168\n",
      "Epoch: 144 \tTrain Loss: 0.02329 \tTrain Acc: 0.9922 \tTest Loss: 0.41567 \tTest Acc: 0.9153 \tTime: 68.2391\n",
      "Epoch: 145 \tTrain Loss: 0.02310 \tTrain Acc: 0.9920 \tTest Loss: 0.41420 \tTest Acc: 0.9114 \tTime: 68.1314\n",
      "Epoch: 146 \tTrain Loss: 0.02249 \tTrain Acc: 0.9926 \tTest Loss: 0.41076 \tTest Acc: 0.9134 \tTime: 68.1698\n",
      "Epoch: 147 \tTrain Loss: 0.02110 \tTrain Acc: 0.9929 \tTest Loss: 0.41071 \tTest Acc: 0.9154 \tTime: 68.1813\n",
      "Epoch: 148 \tTrain Loss: 0.01945 \tTrain Acc: 0.9933 \tTest Loss: 0.40877 \tTest Acc: 0.9203 \tTime: 68.1225\n",
      "Epoch: 149 \tTrain Loss: 0.02195 \tTrain Acc: 0.9922 \tTest Loss: 0.41793 \tTest Acc: 0.9145 \tTime: 68.2124\n",
      "Epoch: 150 \tTrain Loss: 0.02047 \tTrain Acc: 0.9927 \tTest Loss: 0.40126 \tTest Acc: 0.9144 \tTime: 68.1467\n",
      "Epoch: 151 \tTrain Loss: 0.01650 \tTrain Acc: 0.9944 \tTest Loss: 0.41914 \tTest Acc: 0.9163 \tTime: 68.1755\n",
      "Epoch: 152 \tTrain Loss: 0.01920 \tTrain Acc: 0.9936 \tTest Loss: 0.41018 \tTest Acc: 0.9166 \tTime: 68.0957\n",
      "Epoch: 153 \tTrain Loss: 0.01929 \tTrain Acc: 0.9937 \tTest Loss: 0.40671 \tTest Acc: 0.9186 \tTime: 68.2254\n",
      "Epoch: 154 \tTrain Loss: 0.02018 \tTrain Acc: 0.9936 \tTest Loss: 0.41521 \tTest Acc: 0.9166 \tTime: 68.2400\n",
      "Epoch: 155 \tTrain Loss: 0.01883 \tTrain Acc: 0.9936 \tTest Loss: 0.40928 \tTest Acc: 0.9174 \tTime: 68.1572\n",
      "Epoch: 156 \tTrain Loss: 0.02006 \tTrain Acc: 0.9934 \tTest Loss: 0.42471 \tTest Acc: 0.9140 \tTime: 68.1961\n",
      "Epoch: 157 \tTrain Loss: 0.01857 \tTrain Acc: 0.9943 \tTest Loss: 0.41404 \tTest Acc: 0.9171 \tTime: 68.1395\n",
      "Epoch: 158 \tTrain Loss: 0.02090 \tTrain Acc: 0.9933 \tTest Loss: 0.41095 \tTest Acc: 0.9162 \tTime: 68.1763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159 \tTrain Loss: 0.01709 \tTrain Acc: 0.9941 \tTest Loss: 0.41550 \tTest Acc: 0.9163 \tTime: 68.1827\n",
      "Epoch: 160 \tTrain Loss: 0.01863 \tTrain Acc: 0.9938 \tTest Loss: 0.40309 \tTest Acc: 0.9201 \tTime: 67.9982\n",
      "Epoch: 161 \tTrain Loss: 0.01684 \tTrain Acc: 0.9939 \tTest Loss: 0.41879 \tTest Acc: 0.9163 \tTime: 68.2328\n",
      "Epoch: 162 \tTrain Loss: 0.01743 \tTrain Acc: 0.9944 \tTest Loss: 0.42704 \tTest Acc: 0.9201 \tTime: 68.2370\n",
      "Epoch: 163 \tTrain Loss: 0.01790 \tTrain Acc: 0.9939 \tTest Loss: 0.41083 \tTest Acc: 0.9186 \tTime: 68.4048\n",
      "Epoch: 164 \tTrain Loss: 0.01781 \tTrain Acc: 0.9943 \tTest Loss: 0.41055 \tTest Acc: 0.9183 \tTime: 68.2622\n",
      "Epoch: 165 \tTrain Loss: 0.01737 \tTrain Acc: 0.9944 \tTest Loss: 0.41074 \tTest Acc: 0.9179 \tTime: 68.2654\n",
      "Epoch: 166 \tTrain Loss: 0.01720 \tTrain Acc: 0.9944 \tTest Loss: 0.42337 \tTest Acc: 0.9170 \tTime: 68.2485\n",
      "Epoch: 167 \tTrain Loss: 0.01706 \tTrain Acc: 0.9944 \tTest Loss: 0.41821 \tTest Acc: 0.9161 \tTime: 68.3817\n",
      "Epoch: 168 \tTrain Loss: 0.01903 \tTrain Acc: 0.9939 \tTest Loss: 0.42197 \tTest Acc: 0.9188 \tTime: 68.3356\n",
      "Epoch: 169 \tTrain Loss: 0.01938 \tTrain Acc: 0.9937 \tTest Loss: 0.42553 \tTest Acc: 0.9153 \tTime: 68.1777\n",
      "Epoch: 170 \tTrain Loss: 0.01610 \tTrain Acc: 0.9947 \tTest Loss: 0.42491 \tTest Acc: 0.9178 \tTime: 68.1572\n",
      "Epoch: 171 \tTrain Loss: 0.01527 \tTrain Acc: 0.9947 \tTest Loss: 0.42357 \tTest Acc: 0.9177 \tTime: 68.2742\n",
      "Epoch: 172 \tTrain Loss: 0.01634 \tTrain Acc: 0.9947 \tTest Loss: 0.41688 \tTest Acc: 0.9172 \tTime: 68.3549\n",
      "Epoch: 173 \tTrain Loss: 0.01527 \tTrain Acc: 0.9952 \tTest Loss: 0.45123 \tTest Acc: 0.9161 \tTime: 68.3355\n",
      "Epoch: 174 \tTrain Loss: 0.01522 \tTrain Acc: 0.9947 \tTest Loss: 0.43375 \tTest Acc: 0.9160 \tTime: 68.2404\n",
      "Epoch: 175 \tTrain Loss: 0.01594 \tTrain Acc: 0.9943 \tTest Loss: 0.42606 \tTest Acc: 0.9156 \tTime: 68.3727\n",
      "Epoch: 176 \tTrain Loss: 0.01580 \tTrain Acc: 0.9949 \tTest Loss: 0.42690 \tTest Acc: 0.9181 \tTime: 68.1886\n",
      "Epoch: 177 \tTrain Loss: 0.01610 \tTrain Acc: 0.9949 \tTest Loss: 0.42813 \tTest Acc: 0.9154 \tTime: 68.2251\n",
      "Epoch: 178 \tTrain Loss: 0.01514 \tTrain Acc: 0.9948 \tTest Loss: 0.42053 \tTest Acc: 0.9161 \tTime: 68.1020\n",
      "Epoch: 179 \tTrain Loss: 0.01553 \tTrain Acc: 0.9949 \tTest Loss: 0.41913 \tTest Acc: 0.9163 \tTime: 68.1555\n",
      "Epoch: 180 \tTrain Loss: 0.01478 \tTrain Acc: 0.9948 \tTest Loss: 0.42625 \tTest Acc: 0.9188 \tTime: 68.4496\n",
      "Epoch: 181 \tTrain Loss: 0.01403 \tTrain Acc: 0.9950 \tTest Loss: 0.42354 \tTest Acc: 0.9181 \tTime: 68.2838\n",
      "Epoch: 182 \tTrain Loss: 0.01407 \tTrain Acc: 0.9952 \tTest Loss: 0.42209 \tTest Acc: 0.9175 \tTime: 68.2034\n",
      "Epoch: 183 \tTrain Loss: 0.01243 \tTrain Acc: 0.9958 \tTest Loss: 0.43131 \tTest Acc: 0.9172 \tTime: 68.3561\n",
      "Epoch: 184 \tTrain Loss: 0.01360 \tTrain Acc: 0.9954 \tTest Loss: 0.43496 \tTest Acc: 0.9155 \tTime: 68.2619\n",
      "Epoch: 185 \tTrain Loss: 0.01482 \tTrain Acc: 0.9950 \tTest Loss: 0.43418 \tTest Acc: 0.9178 \tTime: 68.0619\n",
      "Epoch: 186 \tTrain Loss: 0.01437 \tTrain Acc: 0.9952 \tTest Loss: 0.43633 \tTest Acc: 0.9162 \tTime: 68.1681\n",
      "Epoch: 187 \tTrain Loss: 0.01289 \tTrain Acc: 0.9956 \tTest Loss: 0.42014 \tTest Acc: 0.9168 \tTime: 68.1105\n",
      "Epoch: 188 \tTrain Loss: 0.01320 \tTrain Acc: 0.9957 \tTest Loss: 0.41922 \tTest Acc: 0.9177 \tTime: 68.1570\n",
      "Epoch: 189 \tTrain Loss: 0.01458 \tTrain Acc: 0.9949 \tTest Loss: 0.42783 \tTest Acc: 0.9158 \tTime: 68.3582\n",
      "Epoch: 190 \tTrain Loss: 0.01218 \tTrain Acc: 0.9958 \tTest Loss: 0.43164 \tTest Acc: 0.9157 \tTime: 68.1888\n",
      "Epoch: 191 \tTrain Loss: 0.01404 \tTrain Acc: 0.9957 \tTest Loss: 0.41915 \tTest Acc: 0.9193 \tTime: 68.1370\n",
      "Epoch: 192 \tTrain Loss: 0.01342 \tTrain Acc: 0.9953 \tTest Loss: 0.42227 \tTest Acc: 0.9175 \tTime: 68.2550\n",
      "Epoch: 193 \tTrain Loss: 0.01462 \tTrain Acc: 0.9949 \tTest Loss: 0.43062 \tTest Acc: 0.9149 \tTime: 68.2141\n",
      "Epoch: 194 \tTrain Loss: 0.01234 \tTrain Acc: 0.9960 \tTest Loss: 0.41889 \tTest Acc: 0.9183 \tTime: 68.2368\n",
      "Epoch: 195 \tTrain Loss: 0.01231 \tTrain Acc: 0.9959 \tTest Loss: 0.42348 \tTest Acc: 0.9201 \tTime: 68.3269\n",
      "Epoch: 196 \tTrain Loss: 0.01192 \tTrain Acc: 0.9958 \tTest Loss: 0.42873 \tTest Acc: 0.9201 \tTime: 68.2101\n",
      "Epoch: 197 \tTrain Loss: 0.01371 \tTrain Acc: 0.9955 \tTest Loss: 0.43377 \tTest Acc: 0.9195 \tTime: 68.1106\n",
      "Epoch: 198 \tTrain Loss: 0.01295 \tTrain Acc: 0.9956 \tTest Loss: 0.44696 \tTest Acc: 0.9150 \tTime: 68.3649\n",
      "Epoch: 199 \tTrain Loss: 0.01245 \tTrain Acc: 0.9959 \tTest Loss: 0.42439 \tTest Acc: 0.9190 \tTime: 68.2407\n",
      "Epoch: 200 \tTrain Loss: 0.01232 \tTrain Acc: 0.9958 \tTest Loss: 0.42902 \tTest Acc: 0.9204 \tTime: 68.2212\n",
      "Epoch: 201 \tTrain Loss: 0.01177 \tTrain Acc: 0.9963 \tTest Loss: 0.43083 \tTest Acc: 0.9184 \tTime: 68.1748\n",
      "Epoch: 202 \tTrain Loss: 0.01101 \tTrain Acc: 0.9964 \tTest Loss: 0.42891 \tTest Acc: 0.9204 \tTime: 68.3494\n",
      "Epoch: 203 \tTrain Loss: 0.01109 \tTrain Acc: 0.9962 \tTest Loss: 0.44154 \tTest Acc: 0.9179 \tTime: 68.1218\n",
      "Epoch: 204 \tTrain Loss: 0.01204 \tTrain Acc: 0.9959 \tTest Loss: 0.45543 \tTest Acc: 0.9176 \tTime: 68.2067\n",
      "Epoch: 205 \tTrain Loss: 0.01135 \tTrain Acc: 0.9964 \tTest Loss: 0.42169 \tTest Acc: 0.9192 \tTime: 68.3187\n",
      "Epoch: 206 \tTrain Loss: 0.01103 \tTrain Acc: 0.9965 \tTest Loss: 0.43411 \tTest Acc: 0.9191 \tTime: 68.3290\n",
      "Epoch: 207 \tTrain Loss: 0.01003 \tTrain Acc: 0.9966 \tTest Loss: 0.44936 \tTest Acc: 0.9196 \tTime: 68.1354\n",
      "Epoch: 208 \tTrain Loss: 0.01062 \tTrain Acc: 0.9965 \tTest Loss: 0.45482 \tTest Acc: 0.9154 \tTime: 66.1799\n",
      "Epoch: 209 \tTrain Loss: 0.01111 \tTrain Acc: 0.9959 \tTest Loss: 0.43507 \tTest Acc: 0.9185 \tTime: 65.8997\n",
      "Epoch: 210 \tTrain Loss: 0.00871 \tTrain Acc: 0.9972 \tTest Loss: 0.44345 \tTest Acc: 0.9191 \tTime: 65.8395\n",
      "Epoch: 211 \tTrain Loss: 0.01124 \tTrain Acc: 0.9962 \tTest Loss: 0.43359 \tTest Acc: 0.9197 \tTime: 65.9280\n",
      "Epoch: 212 \tTrain Loss: 0.01084 \tTrain Acc: 0.9966 \tTest Loss: 0.42852 \tTest Acc: 0.9200 \tTime: 65.9300\n",
      "Epoch: 213 \tTrain Loss: 0.01007 \tTrain Acc: 0.9968 \tTest Loss: 0.44121 \tTest Acc: 0.9179 \tTime: 65.8103\n",
      "Epoch: 214 \tTrain Loss: 0.01022 \tTrain Acc: 0.9967 \tTest Loss: 0.43275 \tTest Acc: 0.9198 \tTime: 65.9131\n",
      "Epoch: 215 \tTrain Loss: 0.00957 \tTrain Acc: 0.9969 \tTest Loss: 0.44075 \tTest Acc: 0.9175 \tTime: 65.9369\n",
      "Epoch: 216 \tTrain Loss: 0.01055 \tTrain Acc: 0.9964 \tTest Loss: 0.43222 \tTest Acc: 0.9185 \tTime: 65.8953\n",
      "Epoch: 217 \tTrain Loss: 0.01067 \tTrain Acc: 0.9964 \tTest Loss: 0.42979 \tTest Acc: 0.9204 \tTime: 65.8741\n",
      "Epoch: 218 \tTrain Loss: 0.01182 \tTrain Acc: 0.9963 \tTest Loss: 0.41982 \tTest Acc: 0.9209 \tTime: 65.8989\n",
      "Epoch: 219 \tTrain Loss: 0.00981 \tTrain Acc: 0.9967 \tTest Loss: 0.42444 \tTest Acc: 0.9218 \tTime: 65.8586\n",
      "Epoch: 220 \tTrain Loss: 0.00959 \tTrain Acc: 0.9968 \tTest Loss: 0.42655 \tTest Acc: 0.9214 \tTime: 65.9193\n",
      "Epoch: 221 \tTrain Loss: 0.00937 \tTrain Acc: 0.9969 \tTest Loss: 0.43357 \tTest Acc: 0.9199 \tTime: 65.9165\n",
      "Epoch: 222 \tTrain Loss: 0.00981 \tTrain Acc: 0.9968 \tTest Loss: 0.42799 \tTest Acc: 0.9207 \tTime: 65.9235\n",
      "Epoch: 223 \tTrain Loss: 0.00929 \tTrain Acc: 0.9970 \tTest Loss: 0.42953 \tTest Acc: 0.9199 \tTime: 65.9227\n",
      "Epoch: 224 \tTrain Loss: 0.00966 \tTrain Acc: 0.9969 \tTest Loss: 0.42701 \tTest Acc: 0.9206 \tTime: 65.9173\n",
      "Epoch: 225 \tTrain Loss: 0.01065 \tTrain Acc: 0.9967 \tTest Loss: 0.42264 \tTest Acc: 0.9192 \tTime: 65.9098\n",
      "Epoch: 226 \tTrain Loss: 0.00953 \tTrain Acc: 0.9968 \tTest Loss: 0.43396 \tTest Acc: 0.9214 \tTime: 65.8333\n",
      "Epoch: 227 \tTrain Loss: 0.00908 \tTrain Acc: 0.9967 \tTest Loss: 0.44099 \tTest Acc: 0.9204 \tTime: 65.9225\n",
      "Epoch: 228 \tTrain Loss: 0.00877 \tTrain Acc: 0.9969 \tTest Loss: 0.44994 \tTest Acc: 0.9197 \tTime: 65.8787\n",
      "Epoch: 229 \tTrain Loss: 0.00830 \tTrain Acc: 0.9973 \tTest Loss: 0.43690 \tTest Acc: 0.9217 \tTime: 65.8785\n",
      "Epoch: 230 \tTrain Loss: 0.01010 \tTrain Acc: 0.9967 \tTest Loss: 0.44006 \tTest Acc: 0.9214 \tTime: 65.8966\n",
      "Epoch: 231 \tTrain Loss: 0.00884 \tTrain Acc: 0.9970 \tTest Loss: 0.43076 \tTest Acc: 0.9216 \tTime: 65.9116\n",
      "Epoch: 232 \tTrain Loss: 0.00992 \tTrain Acc: 0.9968 \tTest Loss: 0.43934 \tTest Acc: 0.9200 \tTime: 65.8541\n",
      "Epoch: 233 \tTrain Loss: 0.00865 \tTrain Acc: 0.9970 \tTest Loss: 0.44061 \tTest Acc: 0.9187 \tTime: 65.9703\n",
      "Epoch: 234 \tTrain Loss: 0.00936 \tTrain Acc: 0.9971 \tTest Loss: 0.43102 \tTest Acc: 0.9214 \tTime: 66.0021\n",
      "Epoch: 235 \tTrain Loss: 0.00923 \tTrain Acc: 0.9970 \tTest Loss: 0.44458 \tTest Acc: 0.9183 \tTime: 65.9303\n",
      "Epoch: 236 \tTrain Loss: 0.00964 \tTrain Acc: 0.9968 \tTest Loss: 0.43533 \tTest Acc: 0.9193 \tTime: 65.8543\n",
      "Epoch: 237 \tTrain Loss: 0.00819 \tTrain Acc: 0.9973 \tTest Loss: 0.43292 \tTest Acc: 0.9216 \tTime: 65.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238 \tTrain Loss: 0.00882 \tTrain Acc: 0.9971 \tTest Loss: 0.44233 \tTest Acc: 0.9200 \tTime: 65.8979\n",
      "Epoch: 239 \tTrain Loss: 0.00807 \tTrain Acc: 0.9972 \tTest Loss: 0.45019 \tTest Acc: 0.9184 \tTime: 65.8929\n",
      "Epoch: 240 \tTrain Loss: 0.00735 \tTrain Acc: 0.9976 \tTest Loss: 0.44341 \tTest Acc: 0.9195 \tTime: 65.9224\n",
      "Epoch: 241 \tTrain Loss: 0.00769 \tTrain Acc: 0.9973 \tTest Loss: 0.44610 \tTest Acc: 0.9178 \tTime: 65.9007\n",
      "Epoch: 242 \tTrain Loss: 0.00772 \tTrain Acc: 0.9976 \tTest Loss: 0.44431 \tTest Acc: 0.9189 \tTime: 65.9864\n",
      "Epoch: 243 \tTrain Loss: 0.00821 \tTrain Acc: 0.9971 \tTest Loss: 0.44196 \tTest Acc: 0.9195 \tTime: 65.8807\n",
      "Epoch: 244 \tTrain Loss: 0.00766 \tTrain Acc: 0.9974 \tTest Loss: 0.44460 \tTest Acc: 0.9203 \tTime: 65.9586\n",
      "Epoch: 245 \tTrain Loss: 0.00814 \tTrain Acc: 0.9975 \tTest Loss: 0.44168 \tTest Acc: 0.9213 \tTime: 65.9537\n",
      "Epoch: 246 \tTrain Loss: 0.00744 \tTrain Acc: 0.9973 \tTest Loss: 0.44529 \tTest Acc: 0.9195 \tTime: 65.9921\n",
      "Epoch: 247 \tTrain Loss: 0.00826 \tTrain Acc: 0.9973 \tTest Loss: 0.44100 \tTest Acc: 0.9192 \tTime: 65.9112\n",
      "Epoch: 248 \tTrain Loss: 0.00863 \tTrain Acc: 0.9969 \tTest Loss: 0.45131 \tTest Acc: 0.9212 \tTime: 65.9614\n",
      "Epoch: 249 \tTrain Loss: 0.00809 \tTrain Acc: 0.9972 \tTest Loss: 0.44369 \tTest Acc: 0.9198 \tTime: 66.0939\n",
      "Epoch: 250 \tTrain Loss: 0.00755 \tTrain Acc: 0.9974 \tTest Loss: 0.45506 \tTest Acc: 0.9166 \tTime: 65.9543\n",
      "Epoch: 251 \tTrain Loss: 0.00774 \tTrain Acc: 0.9976 \tTest Loss: 0.44440 \tTest Acc: 0.9210 \tTime: 66.0201\n",
      "Epoch: 252 \tTrain Loss: 0.00739 \tTrain Acc: 0.9976 \tTest Loss: 0.43832 \tTest Acc: 0.9204 \tTime: 65.9775\n",
      "Epoch: 253 \tTrain Loss: 0.00808 \tTrain Acc: 0.9973 \tTest Loss: 0.44010 \tTest Acc: 0.9187 \tTime: 66.1070\n",
      "Epoch: 254 \tTrain Loss: 0.00820 \tTrain Acc: 0.9972 \tTest Loss: 0.44355 \tTest Acc: 0.9202 \tTime: 66.0872\n",
      "Epoch: 255 \tTrain Loss: 0.00670 \tTrain Acc: 0.9979 \tTest Loss: 0.44648 \tTest Acc: 0.9197 \tTime: 66.0067\n",
      "Epoch: 256 \tTrain Loss: 0.00710 \tTrain Acc: 0.9977 \tTest Loss: 0.43513 \tTest Acc: 0.9221 \tTime: 65.9508\n",
      "Epoch: 257 \tTrain Loss: 0.00782 \tTrain Acc: 0.9976 \tTest Loss: 0.44121 \tTest Acc: 0.9192 \tTime: 65.9494\n",
      "Epoch: 258 \tTrain Loss: 0.00697 \tTrain Acc: 0.9977 \tTest Loss: 0.43594 \tTest Acc: 0.9204 \tTime: 65.9313\n",
      "Epoch: 259 \tTrain Loss: 0.00750 \tTrain Acc: 0.9975 \tTest Loss: 0.44337 \tTest Acc: 0.9205 \tTime: 66.0007\n",
      "Epoch: 260 \tTrain Loss: 0.00633 \tTrain Acc: 0.9981 \tTest Loss: 0.43609 \tTest Acc: 0.9217 \tTime: 65.8861\n",
      "Epoch: 261 \tTrain Loss: 0.00714 \tTrain Acc: 0.9977 \tTest Loss: 0.43385 \tTest Acc: 0.9223 \tTime: 65.9169\n",
      "Epoch: 262 \tTrain Loss: 0.00722 \tTrain Acc: 0.9977 \tTest Loss: 0.42900 \tTest Acc: 0.9248 \tTime: 65.9836\n",
      "Epoch: 263 \tTrain Loss: 0.00771 \tTrain Acc: 0.9974 \tTest Loss: 0.43876 \tTest Acc: 0.9230 \tTime: 65.9010\n",
      "Epoch: 264 \tTrain Loss: 0.00746 \tTrain Acc: 0.9977 \tTest Loss: 0.44184 \tTest Acc: 0.9192 \tTime: 66.5629\n",
      "Epoch: 265 \tTrain Loss: 0.00648 \tTrain Acc: 0.9979 \tTest Loss: 0.44162 \tTest Acc: 0.9216 \tTime: 66.7594\n",
      "Epoch: 266 \tTrain Loss: 0.00722 \tTrain Acc: 0.9977 \tTest Loss: 0.43906 \tTest Acc: 0.9215 \tTime: 66.6758\n",
      "Epoch: 267 \tTrain Loss: 0.00608 \tTrain Acc: 0.9982 \tTest Loss: 0.44162 \tTest Acc: 0.9220 \tTime: 66.0768\n",
      "Epoch: 268 \tTrain Loss: 0.00702 \tTrain Acc: 0.9975 \tTest Loss: 0.44697 \tTest Acc: 0.9231 \tTime: 65.1989\n",
      "Epoch: 269 \tTrain Loss: 0.00610 \tTrain Acc: 0.9980 \tTest Loss: 0.45041 \tTest Acc: 0.9202 \tTime: 65.0771\n",
      "Epoch: 270 \tTrain Loss: 0.00712 \tTrain Acc: 0.9979 \tTest Loss: 0.44481 \tTest Acc: 0.9213 \tTime: 64.9797\n",
      "Epoch: 271 \tTrain Loss: 0.00658 \tTrain Acc: 0.9978 \tTest Loss: 0.44896 \tTest Acc: 0.9217 \tTime: 64.9551\n",
      "Epoch: 272 \tTrain Loss: 0.00748 \tTrain Acc: 0.9975 \tTest Loss: 0.44262 \tTest Acc: 0.9223 \tTime: 65.0241\n",
      "Epoch: 273 \tTrain Loss: 0.00689 \tTrain Acc: 0.9979 \tTest Loss: 0.45951 \tTest Acc: 0.9203 \tTime: 65.0071\n",
      "Epoch: 274 \tTrain Loss: 0.00563 \tTrain Acc: 0.9983 \tTest Loss: 0.45928 \tTest Acc: 0.9209 \tTime: 65.0056\n",
      "Epoch: 275 \tTrain Loss: 0.00679 \tTrain Acc: 0.9979 \tTest Loss: 0.45572 \tTest Acc: 0.9202 \tTime: 64.9564\n",
      "Epoch: 276 \tTrain Loss: 0.00705 \tTrain Acc: 0.9978 \tTest Loss: 0.44576 \tTest Acc: 0.9212 \tTime: 64.9419\n",
      "Epoch: 277 \tTrain Loss: 0.00700 \tTrain Acc: 0.9979 \tTest Loss: 0.44991 \tTest Acc: 0.9204 \tTime: 64.9983\n",
      "Epoch: 278 \tTrain Loss: 0.00592 \tTrain Acc: 0.9980 \tTest Loss: 0.44465 \tTest Acc: 0.9217 \tTime: 65.2404\n",
      "Epoch: 279 \tTrain Loss: 0.00669 \tTrain Acc: 0.9979 \tTest Loss: 0.43984 \tTest Acc: 0.9217 \tTime: 65.6209\n",
      "Epoch: 280 \tTrain Loss: 0.00623 \tTrain Acc: 0.9978 \tTest Loss: 0.44239 \tTest Acc: 0.9202 \tTime: 65.0188\n",
      "Epoch: 281 \tTrain Loss: 0.00599 \tTrain Acc: 0.9981 \tTest Loss: 0.43935 \tTest Acc: 0.9222 \tTime: 65.6807\n",
      "Epoch: 282 \tTrain Loss: 0.00640 \tTrain Acc: 0.9981 \tTest Loss: 0.43371 \tTest Acc: 0.9241 \tTime: 66.5017\n",
      "Epoch: 283 \tTrain Loss: 0.00613 \tTrain Acc: 0.9981 \tTest Loss: 0.43840 \tTest Acc: 0.9238 \tTime: 65.4695\n",
      "Epoch: 284 \tTrain Loss: 0.00639 \tTrain Acc: 0.9981 \tTest Loss: 0.44172 \tTest Acc: 0.9233 \tTime: 65.0781\n",
      "Epoch: 285 \tTrain Loss: 0.00587 \tTrain Acc: 0.9980 \tTest Loss: 0.44153 \tTest Acc: 0.9245 \tTime: 65.0696\n",
      "Epoch: 286 \tTrain Loss: 0.00604 \tTrain Acc: 0.9980 \tTest Loss: 0.43728 \tTest Acc: 0.9243 \tTime: 65.0440\n",
      "Epoch: 287 \tTrain Loss: 0.00605 \tTrain Acc: 0.9981 \tTest Loss: 0.44150 \tTest Acc: 0.9235 \tTime: 65.2205\n",
      "Epoch: 288 \tTrain Loss: 0.00675 \tTrain Acc: 0.9980 \tTest Loss: 0.44264 \tTest Acc: 0.9215 \tTime: 65.3432\n",
      "Epoch: 289 \tTrain Loss: 0.00629 \tTrain Acc: 0.9979 \tTest Loss: 0.44038 \tTest Acc: 0.9224 \tTime: 65.3815\n",
      "Epoch: 290 \tTrain Loss: 0.00581 \tTrain Acc: 0.9980 \tTest Loss: 0.44024 \tTest Acc: 0.9204 \tTime: 65.4643\n",
      "Epoch: 291 \tTrain Loss: 0.00652 \tTrain Acc: 0.9978 \tTest Loss: 0.44158 \tTest Acc: 0.9228 \tTime: 65.4410\n",
      "Epoch: 292 \tTrain Loss: 0.00570 \tTrain Acc: 0.9981 \tTest Loss: 0.43542 \tTest Acc: 0.9230 \tTime: 65.4561\n",
      "Epoch: 293 \tTrain Loss: 0.00599 \tTrain Acc: 0.9978 \tTest Loss: 0.43048 \tTest Acc: 0.9240 \tTime: 65.3781\n",
      "Epoch: 294 \tTrain Loss: 0.00609 \tTrain Acc: 0.9980 \tTest Loss: 0.44297 \tTest Acc: 0.9224 \tTime: 65.3726\n",
      "Epoch: 295 \tTrain Loss: 0.00573 \tTrain Acc: 0.9981 \tTest Loss: 0.44843 \tTest Acc: 0.9215 \tTime: 65.4044\n",
      "Epoch: 296 \tTrain Loss: 0.00566 \tTrain Acc: 0.9981 \tTest Loss: 0.44385 \tTest Acc: 0.9228 \tTime: 65.4201\n",
      "Epoch: 297 \tTrain Loss: 0.00559 \tTrain Acc: 0.9981 \tTest Loss: 0.44834 \tTest Acc: 0.9214 \tTime: 65.3602\n",
      "Epoch: 298 \tTrain Loss: 0.00588 \tTrain Acc: 0.9980 \tTest Loss: 0.44761 \tTest Acc: 0.9226 \tTime: 65.3532\n",
      "Epoch: 299 \tTrain Loss: 0.00574 \tTrain Acc: 0.9980 \tTest Loss: 0.44612 \tTest Acc: 0.9222 \tTime: 65.4033\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "Nepoch = 300\n",
    "\n",
    "for epoch in range(Nepoch): \n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    t1 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainDataLoader):\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = net(images)\n",
    "        fit = Loss(predicted_output,labels)\n",
    "        fit.backward()\n",
    "        optimizer.step()   \n",
    "        train_loss += fit.item()\n",
    "        train_acc += torch.sum(labels == predicted_output.argmax(dim=1)).item()\n",
    "\n",
    "    for i, data in enumerate(testDataLoader):\n",
    "        with torch.no_grad():\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            predicted_output = net(images)\n",
    "            fit = Loss(predicted_output,labels)\n",
    "            test_loss += fit.item()\n",
    "            test_acc += torch.sum(labels == predicted_output.argmax(dim=1)).item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = train_loss/len(trainDataLoader)\n",
    "    test_loss = test_loss/len(testDataLoader)\n",
    "    train_acc = train_acc/len(trainingdata)\n",
    "    test_acc = test_acc/len(testdata)\n",
    "    train_acc_history.append(train_acc)\n",
    "    test_acc_history.append(test_acc)\n",
    "    t2 = time.time()\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\tTrain Loss: {train_loss:.5f} \\tTrain Acc: {train_acc:.4f} \\tTest Loss: {test_loss:.5f} \\tTest Acc: {test_acc:.4f} \\tTime: {t2-t1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 \tTrain Loss: 0.00608 \tTrain Acc: 0.9980 \tTest Loss: 0.44175 \tTest Acc: 0.9239 \tTime: 64.9735\n",
      "Epoch: 301 \tTrain Loss: 0.00497 \tTrain Acc: 0.9983 \tTest Loss: 0.44050 \tTest Acc: 0.9242 \tTime: 61.9716\n",
      "Epoch: 302 \tTrain Loss: 0.00645 \tTrain Acc: 0.9978 \tTest Loss: 0.44504 \tTest Acc: 0.9241 \tTime: 62.0401\n",
      "Epoch: 303 \tTrain Loss: 0.00626 \tTrain Acc: 0.9979 \tTest Loss: 0.43716 \tTest Acc: 0.9251 \tTime: 62.1443\n",
      "Epoch: 304 \tTrain Loss: 0.00540 \tTrain Acc: 0.9982 \tTest Loss: 0.44094 \tTest Acc: 0.9244 \tTime: 65.4495\n",
      "Epoch: 305 \tTrain Loss: 0.00620 \tTrain Acc: 0.9979 \tTest Loss: 0.44247 \tTest Acc: 0.9239 \tTime: 66.7454\n",
      "Epoch: 306 \tTrain Loss: 0.00582 \tTrain Acc: 0.9980 \tTest Loss: 0.44460 \tTest Acc: 0.9226 \tTime: 66.6471\n",
      "Epoch: 307 \tTrain Loss: 0.00556 \tTrain Acc: 0.9982 \tTest Loss: 0.44018 \tTest Acc: 0.9242 \tTime: 66.6326\n",
      "Epoch: 308 \tTrain Loss: 0.00553 \tTrain Acc: 0.9981 \tTest Loss: 0.43835 \tTest Acc: 0.9249 \tTime: 66.6868\n",
      "Epoch: 309 \tTrain Loss: 0.00483 \tTrain Acc: 0.9985 \tTest Loss: 0.45232 \tTest Acc: 0.9230 \tTime: 66.7209\n",
      "Epoch: 310 \tTrain Loss: 0.00523 \tTrain Acc: 0.9982 \tTest Loss: 0.44465 \tTest Acc: 0.9237 \tTime: 65.2354\n",
      "Epoch: 311 \tTrain Loss: 0.00541 \tTrain Acc: 0.9983 \tTest Loss: 0.44935 \tTest Acc: 0.9232 \tTime: 64.4034\n",
      "Epoch: 312 \tTrain Loss: 0.00550 \tTrain Acc: 0.9983 \tTest Loss: 0.45015 \tTest Acc: 0.9233 \tTime: 64.3084\n",
      "Epoch: 313 \tTrain Loss: 0.00528 \tTrain Acc: 0.9984 \tTest Loss: 0.45242 \tTest Acc: 0.9226 \tTime: 65.0409\n",
      "Epoch: 314 \tTrain Loss: 0.00504 \tTrain Acc: 0.9985 \tTest Loss: 0.45376 \tTest Acc: 0.9203 \tTime: 67.8087\n",
      "Epoch: 315 \tTrain Loss: 0.00541 \tTrain Acc: 0.9981 \tTest Loss: 0.45021 \tTest Acc: 0.9237 \tTime: 68.1529\n",
      "Epoch: 316 \tTrain Loss: 0.00443 \tTrain Acc: 0.9984 \tTest Loss: 0.45263 \tTest Acc: 0.9234 \tTime: 68.8272\n",
      "Epoch: 317 \tTrain Loss: 0.00556 \tTrain Acc: 0.9981 \tTest Loss: 0.45037 \tTest Acc: 0.9244 \tTime: 68.9324\n",
      "Epoch: 318 \tTrain Loss: 0.00503 \tTrain Acc: 0.9986 \tTest Loss: 0.45411 \tTest Acc: 0.9234 \tTime: 68.2640\n",
      "Epoch: 319 \tTrain Loss: 0.00547 \tTrain Acc: 0.9981 \tTest Loss: 0.44401 \tTest Acc: 0.9246 \tTime: 65.4433\n",
      "Epoch: 320 \tTrain Loss: 0.00539 \tTrain Acc: 0.9982 \tTest Loss: 0.44684 \tTest Acc: 0.9239 \tTime: 63.8501\n",
      "Epoch: 321 \tTrain Loss: 0.00567 \tTrain Acc: 0.9981 \tTest Loss: 0.44412 \tTest Acc: 0.9251 \tTime: 63.7542\n",
      "Epoch: 322 \tTrain Loss: 0.00515 \tTrain Acc: 0.9983 \tTest Loss: 0.44528 \tTest Acc: 0.9224 \tTime: 63.7996\n",
      "Epoch: 323 \tTrain Loss: 0.00563 \tTrain Acc: 0.9982 \tTest Loss: 0.44358 \tTest Acc: 0.9245 \tTime: 62.7756\n",
      "Epoch: 324 \tTrain Loss: 0.00484 \tTrain Acc: 0.9984 \tTest Loss: 0.44582 \tTest Acc: 0.9236 \tTime: 61.9329\n",
      "Epoch: 325 \tTrain Loss: 0.00507 \tTrain Acc: 0.9981 \tTest Loss: 0.44386 \tTest Acc: 0.9240 \tTime: 65.0055\n",
      "Epoch: 326 \tTrain Loss: 0.00516 \tTrain Acc: 0.9984 \tTest Loss: 0.43785 \tTest Acc: 0.9236 \tTime: 64.0557\n",
      "Epoch: 327 \tTrain Loss: 0.00418 \tTrain Acc: 0.9987 \tTest Loss: 0.44416 \tTest Acc: 0.9239 \tTime: 64.5319\n",
      "Epoch: 328 \tTrain Loss: 0.00476 \tTrain Acc: 0.9985 \tTest Loss: 0.44659 \tTest Acc: 0.9248 \tTime: 66.5152\n",
      "Epoch: 329 \tTrain Loss: 0.00569 \tTrain Acc: 0.9982 \tTest Loss: 0.44131 \tTest Acc: 0.9264 \tTime: 68.9609\n",
      "Epoch: 330 \tTrain Loss: 0.00457 \tTrain Acc: 0.9984 \tTest Loss: 0.44253 \tTest Acc: 0.9248 \tTime: 68.1211\n",
      "Epoch: 331 \tTrain Loss: 0.00476 \tTrain Acc: 0.9983 \tTest Loss: 0.44351 \tTest Acc: 0.9245 \tTime: 68.5823\n",
      "Epoch: 332 \tTrain Loss: 0.00498 \tTrain Acc: 0.9986 \tTest Loss: 0.44208 \tTest Acc: 0.9247 \tTime: 68.7207\n",
      "Epoch: 333 \tTrain Loss: 0.00431 \tTrain Acc: 0.9987 \tTest Loss: 0.45032 \tTest Acc: 0.9233 \tTime: 68.5519\n",
      "Epoch: 334 \tTrain Loss: 0.00493 \tTrain Acc: 0.9982 \tTest Loss: 0.44909 \tTest Acc: 0.9240 \tTime: 68.6485\n",
      "Epoch: 335 \tTrain Loss: 0.00532 \tTrain Acc: 0.9983 \tTest Loss: 0.44650 \tTest Acc: 0.9246 \tTime: 68.5761\n",
      "Epoch: 336 \tTrain Loss: 0.00593 \tTrain Acc: 0.9982 \tTest Loss: 0.44342 \tTest Acc: 0.9245 \tTime: 68.5976\n",
      "Epoch: 337 \tTrain Loss: 0.00428 \tTrain Acc: 0.9986 \tTest Loss: 0.44175 \tTest Acc: 0.9248 \tTime: 68.5727\n",
      "Epoch: 338 \tTrain Loss: 0.00422 \tTrain Acc: 0.9987 \tTest Loss: 0.44149 \tTest Acc: 0.9247 \tTime: 68.4467\n",
      "Epoch: 339 \tTrain Loss: 0.00527 \tTrain Acc: 0.9984 \tTest Loss: 0.44466 \tTest Acc: 0.9242 \tTime: 68.6124\n",
      "Epoch: 340 \tTrain Loss: 0.00465 \tTrain Acc: 0.9985 \tTest Loss: 0.44583 \tTest Acc: 0.9241 \tTime: 68.6657\n",
      "Epoch: 341 \tTrain Loss: 0.00460 \tTrain Acc: 0.9983 \tTest Loss: 0.45493 \tTest Acc: 0.9216 \tTime: 68.5289\n",
      "Epoch: 342 \tTrain Loss: 0.00450 \tTrain Acc: 0.9987 \tTest Loss: 0.44674 \tTest Acc: 0.9228 \tTime: 68.4807\n",
      "Epoch: 343 \tTrain Loss: 0.00509 \tTrain Acc: 0.9984 \tTest Loss: 0.44202 \tTest Acc: 0.9236 \tTime: 68.5474\n",
      "Epoch: 344 \tTrain Loss: 0.00479 \tTrain Acc: 0.9984 \tTest Loss: 0.44169 \tTest Acc: 0.9221 \tTime: 68.5734\n",
      "Epoch: 345 \tTrain Loss: 0.00490 \tTrain Acc: 0.9983 \tTest Loss: 0.44382 \tTest Acc: 0.9249 \tTime: 68.4993\n",
      "Epoch: 346 \tTrain Loss: 0.00423 \tTrain Acc: 0.9987 \tTest Loss: 0.44160 \tTest Acc: 0.9233 \tTime: 68.5686\n",
      "Epoch: 347 \tTrain Loss: 0.00440 \tTrain Acc: 0.9986 \tTest Loss: 0.44226 \tTest Acc: 0.9245 \tTime: 68.5997\n",
      "Epoch: 348 \tTrain Loss: 0.00496 \tTrain Acc: 0.9985 \tTest Loss: 0.44353 \tTest Acc: 0.9247 \tTime: 68.5652\n",
      "Epoch: 349 \tTrain Loss: 0.00516 \tTrain Acc: 0.9984 \tTest Loss: 0.43982 \tTest Acc: 0.9249 \tTime: 68.5087\n",
      "Epoch: 350 \tTrain Loss: 0.00447 \tTrain Acc: 0.9985 \tTest Loss: 0.44131 \tTest Acc: 0.9253 \tTime: 68.5723\n",
      "Epoch: 351 \tTrain Loss: 0.00430 \tTrain Acc: 0.9987 \tTest Loss: 0.44122 \tTest Acc: 0.9265 \tTime: 68.6787\n",
      "Epoch: 352 \tTrain Loss: 0.00433 \tTrain Acc: 0.9986 \tTest Loss: 0.44200 \tTest Acc: 0.9255 \tTime: 68.7105\n",
      "Epoch: 353 \tTrain Loss: 0.00434 \tTrain Acc: 0.9986 \tTest Loss: 0.44438 \tTest Acc: 0.9254 \tTime: 68.4740\n",
      "Epoch: 354 \tTrain Loss: 0.00424 \tTrain Acc: 0.9987 \tTest Loss: 0.44243 \tTest Acc: 0.9258 \tTime: 68.7378\n",
      "Epoch: 355 \tTrain Loss: 0.00465 \tTrain Acc: 0.9986 \tTest Loss: 0.44097 \tTest Acc: 0.9251 \tTime: 68.4969\n",
      "Epoch: 356 \tTrain Loss: 0.00506 \tTrain Acc: 0.9985 \tTest Loss: 0.44132 \tTest Acc: 0.9252 \tTime: 68.5973\n",
      "Epoch: 357 \tTrain Loss: 0.00421 \tTrain Acc: 0.9987 \tTest Loss: 0.44117 \tTest Acc: 0.9242 \tTime: 68.7713\n",
      "Epoch: 358 \tTrain Loss: 0.00539 \tTrain Acc: 0.9982 \tTest Loss: 0.43940 \tTest Acc: 0.9256 \tTime: 68.6935\n",
      "Epoch: 359 \tTrain Loss: 0.00447 \tTrain Acc: 0.9988 \tTest Loss: 0.44079 \tTest Acc: 0.9246 \tTime: 68.6416\n",
      "Epoch: 360 \tTrain Loss: 0.00398 \tTrain Acc: 0.9989 \tTest Loss: 0.44377 \tTest Acc: 0.9231 \tTime: 68.5141\n",
      "Epoch: 361 \tTrain Loss: 0.00399 \tTrain Acc: 0.9987 \tTest Loss: 0.44336 \tTest Acc: 0.9247 \tTime: 68.5731\n",
      "Epoch: 362 \tTrain Loss: 0.00420 \tTrain Acc: 0.9985 \tTest Loss: 0.44483 \tTest Acc: 0.9251 \tTime: 68.6033\n",
      "Epoch: 363 \tTrain Loss: 0.00435 \tTrain Acc: 0.9986 \tTest Loss: 0.44505 \tTest Acc: 0.9245 \tTime: 68.5180\n",
      "Epoch: 364 \tTrain Loss: 0.00358 \tTrain Acc: 0.9989 \tTest Loss: 0.44445 \tTest Acc: 0.9238 \tTime: 68.5126\n",
      "Epoch: 365 \tTrain Loss: 0.00447 \tTrain Acc: 0.9985 \tTest Loss: 0.44228 \tTest Acc: 0.9254 \tTime: 68.5556\n",
      "Epoch: 366 \tTrain Loss: 0.00439 \tTrain Acc: 0.9986 \tTest Loss: 0.44555 \tTest Acc: 0.9251 \tTime: 68.7034\n",
      "Epoch: 367 \tTrain Loss: 0.00403 \tTrain Acc: 0.9985 \tTest Loss: 0.44277 \tTest Acc: 0.9240 \tTime: 68.6750\n",
      "Epoch: 368 \tTrain Loss: 0.00410 \tTrain Acc: 0.9986 \tTest Loss: 0.44727 \tTest Acc: 0.9245 \tTime: 68.4955\n",
      "Epoch: 369 \tTrain Loss: 0.00393 \tTrain Acc: 0.9988 \tTest Loss: 0.44668 \tTest Acc: 0.9243 \tTime: 68.6298\n",
      "Epoch: 370 \tTrain Loss: 0.00448 \tTrain Acc: 0.9985 \tTest Loss: 0.44878 \tTest Acc: 0.9236 \tTime: 68.6502\n",
      "Epoch: 371 \tTrain Loss: 0.00456 \tTrain Acc: 0.9984 \tTest Loss: 0.45792 \tTest Acc: 0.9218 \tTime: 68.7144\n",
      "Epoch: 372 \tTrain Loss: 0.00378 \tTrain Acc: 0.9988 \tTest Loss: 0.45789 \tTest Acc: 0.9221 \tTime: 68.6921\n",
      "Epoch: 373 \tTrain Loss: 0.00356 \tTrain Acc: 0.9990 \tTest Loss: 0.45362 \tTest Acc: 0.9225 \tTime: 68.6372\n",
      "Epoch: 374 \tTrain Loss: 0.00444 \tTrain Acc: 0.9984 \tTest Loss: 0.45364 \tTest Acc: 0.9223 \tTime: 68.6577\n",
      "Epoch: 375 \tTrain Loss: 0.00390 \tTrain Acc: 0.9987 \tTest Loss: 0.44315 \tTest Acc: 0.9240 \tTime: 68.5922\n",
      "Epoch: 376 \tTrain Loss: 0.00381 \tTrain Acc: 0.9988 \tTest Loss: 0.44327 \tTest Acc: 0.9243 \tTime: 68.6721\n",
      "Epoch: 377 \tTrain Loss: 0.00365 \tTrain Acc: 0.9986 \tTest Loss: 0.44573 \tTest Acc: 0.9241 \tTime: 68.6659\n",
      "Epoch: 378 \tTrain Loss: 0.00390 \tTrain Acc: 0.9989 \tTest Loss: 0.44217 \tTest Acc: 0.9245 \tTime: 68.5890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 379 \tTrain Loss: 0.00409 \tTrain Acc: 0.9984 \tTest Loss: 0.44529 \tTest Acc: 0.9238 \tTime: 68.7467\n",
      "Epoch: 380 \tTrain Loss: 0.00470 \tTrain Acc: 0.9983 \tTest Loss: 0.44033 \tTest Acc: 0.9236 \tTime: 68.6571\n",
      "Epoch: 381 \tTrain Loss: 0.00383 \tTrain Acc: 0.9988 \tTest Loss: 0.44122 \tTest Acc: 0.9237 \tTime: 68.7316\n",
      "Epoch: 382 \tTrain Loss: 0.00370 \tTrain Acc: 0.9988 \tTest Loss: 0.44548 \tTest Acc: 0.9243 \tTime: 68.5897\n",
      "Epoch: 383 \tTrain Loss: 0.00382 \tTrain Acc: 0.9987 \tTest Loss: 0.44310 \tTest Acc: 0.9233 \tTime: 68.7006\n",
      "Epoch: 384 \tTrain Loss: 0.00359 \tTrain Acc: 0.9989 \tTest Loss: 0.44318 \tTest Acc: 0.9242 \tTime: 68.7163\n",
      "Epoch: 385 \tTrain Loss: 0.00387 \tTrain Acc: 0.9986 \tTest Loss: 0.44663 \tTest Acc: 0.9239 \tTime: 68.7296\n",
      "Epoch: 386 \tTrain Loss: 0.00375 \tTrain Acc: 0.9989 \tTest Loss: 0.44936 \tTest Acc: 0.9232 \tTime: 68.6231\n",
      "Epoch: 387 \tTrain Loss: 0.00388 \tTrain Acc: 0.9988 \tTest Loss: 0.44834 \tTest Acc: 0.9226 \tTime: 68.5079\n",
      "Epoch: 388 \tTrain Loss: 0.00382 \tTrain Acc: 0.9988 \tTest Loss: 0.44615 \tTest Acc: 0.9230 \tTime: 68.6661\n",
      "Epoch: 389 \tTrain Loss: 0.00336 \tTrain Acc: 0.9989 \tTest Loss: 0.44824 \tTest Acc: 0.9234 \tTime: 68.7551\n",
      "Epoch: 390 \tTrain Loss: 0.00452 \tTrain Acc: 0.9985 \tTest Loss: 0.44732 \tTest Acc: 0.9230 \tTime: 68.6692\n",
      "Epoch: 391 \tTrain Loss: 0.00375 \tTrain Acc: 0.9989 \tTest Loss: 0.44619 \tTest Acc: 0.9234 \tTime: 68.7557\n",
      "Epoch: 392 \tTrain Loss: 0.00371 \tTrain Acc: 0.9988 \tTest Loss: 0.44540 \tTest Acc: 0.9240 \tTime: 68.6915\n",
      "Epoch: 393 \tTrain Loss: 0.00367 \tTrain Acc: 0.9986 \tTest Loss: 0.44795 \tTest Acc: 0.9237 \tTime: 68.6404\n",
      "Epoch: 394 \tTrain Loss: 0.00410 \tTrain Acc: 0.9987 \tTest Loss: 0.44660 \tTest Acc: 0.9248 \tTime: 68.5756\n",
      "Epoch: 395 \tTrain Loss: 0.00405 \tTrain Acc: 0.9986 \tTest Loss: 0.44566 \tTest Acc: 0.9257 \tTime: 68.5310\n",
      "Epoch: 396 \tTrain Loss: 0.00452 \tTrain Acc: 0.9983 \tTest Loss: 0.44937 \tTest Acc: 0.9246 \tTime: 68.6680\n",
      "Epoch: 397 \tTrain Loss: 0.00412 \tTrain Acc: 0.9987 \tTest Loss: 0.44960 \tTest Acc: 0.9235 \tTime: 68.6393\n",
      "Epoch: 398 \tTrain Loss: 0.00346 \tTrain Acc: 0.9989 \tTest Loss: 0.45074 \tTest Acc: 0.9236 \tTime: 68.8555\n",
      "Epoch: 399 \tTrain Loss: 0.00442 \tTrain Acc: 0.9986 \tTest Loss: 0.44909 \tTest Acc: 0.9238 \tTime: 67.2869\n",
      "Epoch: 400 \tTrain Loss: 0.00352 \tTrain Acc: 0.9990 \tTest Loss: 0.44515 \tTest Acc: 0.9244 \tTime: 66.5699\n",
      "Epoch: 401 \tTrain Loss: 0.00428 \tTrain Acc: 0.9986 \tTest Loss: 0.44730 \tTest Acc: 0.9250 \tTime: 67.1554\n",
      "Epoch: 402 \tTrain Loss: 0.00401 \tTrain Acc: 0.9987 \tTest Loss: 0.44738 \tTest Acc: 0.9245 \tTime: 66.6708\n",
      "Epoch: 403 \tTrain Loss: 0.00367 \tTrain Acc: 0.9988 \tTest Loss: 0.44631 \tTest Acc: 0.9246 \tTime: 66.5453\n",
      "Epoch: 404 \tTrain Loss: 0.00336 \tTrain Acc: 0.9990 \tTest Loss: 0.44412 \tTest Acc: 0.9263 \tTime: 66.5795\n",
      "Epoch: 405 \tTrain Loss: 0.00372 \tTrain Acc: 0.9988 \tTest Loss: 0.44152 \tTest Acc: 0.9256 \tTime: 66.5179\n",
      "Epoch: 406 \tTrain Loss: 0.00413 \tTrain Acc: 0.9988 \tTest Loss: 0.44552 \tTest Acc: 0.9242 \tTime: 68.3631\n",
      "Epoch: 407 \tTrain Loss: 0.00353 \tTrain Acc: 0.9990 \tTest Loss: 0.44535 \tTest Acc: 0.9252 \tTime: 68.4676\n",
      "Epoch: 408 \tTrain Loss: 0.00361 \tTrain Acc: 0.9988 \tTest Loss: 0.44441 \tTest Acc: 0.9245 \tTime: 68.3391\n",
      "Epoch: 409 \tTrain Loss: 0.00401 \tTrain Acc: 0.9987 \tTest Loss: 0.44127 \tTest Acc: 0.9259 \tTime: 68.3406\n",
      "Epoch: 410 \tTrain Loss: 0.00335 \tTrain Acc: 0.9990 \tTest Loss: 0.44050 \tTest Acc: 0.9257 \tTime: 68.4921\n",
      "Epoch: 411 \tTrain Loss: 0.00323 \tTrain Acc: 0.9989 \tTest Loss: 0.43957 \tTest Acc: 0.9257 \tTime: 68.4841\n",
      "Epoch: 412 \tTrain Loss: 0.00317 \tTrain Acc: 0.9990 \tTest Loss: 0.44337 \tTest Acc: 0.9246 \tTime: 68.4168\n",
      "Epoch: 413 \tTrain Loss: 0.00420 \tTrain Acc: 0.9987 \tTest Loss: 0.44414 \tTest Acc: 0.9239 \tTime: 68.3767\n",
      "Epoch: 414 \tTrain Loss: 0.00366 \tTrain Acc: 0.9989 \tTest Loss: 0.44401 \tTest Acc: 0.9242 \tTime: 68.4988\n",
      "Epoch: 415 \tTrain Loss: 0.00334 \tTrain Acc: 0.9989 \tTest Loss: 0.44824 \tTest Acc: 0.9242 \tTime: 68.5280\n",
      "Epoch: 416 \tTrain Loss: 0.00338 \tTrain Acc: 0.9988 \tTest Loss: 0.44426 \tTest Acc: 0.9255 \tTime: 68.3743\n",
      "Epoch: 417 \tTrain Loss: 0.00376 \tTrain Acc: 0.9988 \tTest Loss: 0.44406 \tTest Acc: 0.9234 \tTime: 68.2709\n",
      "Epoch: 418 \tTrain Loss: 0.00349 \tTrain Acc: 0.9989 \tTest Loss: 0.44806 \tTest Acc: 0.9232 \tTime: 68.3386\n",
      "Epoch: 419 \tTrain Loss: 0.00366 \tTrain Acc: 0.9987 \tTest Loss: 0.44447 \tTest Acc: 0.9238 \tTime: 68.4340\n",
      "Epoch: 420 \tTrain Loss: 0.00394 \tTrain Acc: 0.9986 \tTest Loss: 0.44490 \tTest Acc: 0.9241 \tTime: 68.4836\n",
      "Epoch: 421 \tTrain Loss: 0.00340 \tTrain Acc: 0.9990 \tTest Loss: 0.44508 \tTest Acc: 0.9235 \tTime: 68.2133\n",
      "Epoch: 422 \tTrain Loss: 0.00295 \tTrain Acc: 0.9990 \tTest Loss: 0.44474 \tTest Acc: 0.9247 \tTime: 68.2753\n",
      "Epoch: 423 \tTrain Loss: 0.00358 \tTrain Acc: 0.9989 \tTest Loss: 0.44553 \tTest Acc: 0.9242 \tTime: 68.3929\n",
      "Epoch: 424 \tTrain Loss: 0.00319 \tTrain Acc: 0.9990 \tTest Loss: 0.44784 \tTest Acc: 0.9234 \tTime: 68.2107\n",
      "Epoch: 425 \tTrain Loss: 0.00355 \tTrain Acc: 0.9989 \tTest Loss: 0.45098 \tTest Acc: 0.9226 \tTime: 68.2781\n",
      "Epoch: 426 \tTrain Loss: 0.00317 \tTrain Acc: 0.9990 \tTest Loss: 0.45212 \tTest Acc: 0.9228 \tTime: 68.2212\n",
      "Epoch: 427 \tTrain Loss: 0.00401 \tTrain Acc: 0.9986 \tTest Loss: 0.44796 \tTest Acc: 0.9235 \tTime: 68.3267\n",
      "Epoch: 428 \tTrain Loss: 0.00428 \tTrain Acc: 0.9987 \tTest Loss: 0.44881 \tTest Acc: 0.9243 \tTime: 68.3497\n",
      "Epoch: 429 \tTrain Loss: 0.00332 \tTrain Acc: 0.9990 \tTest Loss: 0.44715 \tTest Acc: 0.9238 \tTime: 68.3056\n",
      "Epoch: 430 \tTrain Loss: 0.00320 \tTrain Acc: 0.9989 \tTest Loss: 0.45010 \tTest Acc: 0.9238 \tTime: 68.4900\n",
      "Epoch: 431 \tTrain Loss: 0.00409 \tTrain Acc: 0.9988 \tTest Loss: 0.44545 \tTest Acc: 0.9246 \tTime: 68.4376\n",
      "Epoch: 432 \tTrain Loss: 0.00340 \tTrain Acc: 0.9989 \tTest Loss: 0.44665 \tTest Acc: 0.9242 \tTime: 68.2863\n",
      "Epoch: 433 \tTrain Loss: 0.00341 \tTrain Acc: 0.9988 \tTest Loss: 0.44495 \tTest Acc: 0.9242 \tTime: 68.2788\n",
      "Epoch: 434 \tTrain Loss: 0.00403 \tTrain Acc: 0.9986 \tTest Loss: 0.44768 \tTest Acc: 0.9246 \tTime: 68.3338\n",
      "Epoch: 435 \tTrain Loss: 0.00311 \tTrain Acc: 0.9990 \tTest Loss: 0.44510 \tTest Acc: 0.9234 \tTime: 68.2760\n",
      "Epoch: 436 \tTrain Loss: 0.00393 \tTrain Acc: 0.9990 \tTest Loss: 0.44494 \tTest Acc: 0.9251 \tTime: 68.3383\n",
      "Epoch: 437 \tTrain Loss: 0.00306 \tTrain Acc: 0.9989 \tTest Loss: 0.44312 \tTest Acc: 0.9245 \tTime: 68.4307\n",
      "Epoch: 438 \tTrain Loss: 0.00355 \tTrain Acc: 0.9990 \tTest Loss: 0.44266 \tTest Acc: 0.9252 \tTime: 68.4076\n",
      "Epoch: 439 \tTrain Loss: 0.00399 \tTrain Acc: 0.9988 \tTest Loss: 0.44379 \tTest Acc: 0.9247 \tTime: 68.3868\n",
      "Epoch: 440 \tTrain Loss: 0.00341 \tTrain Acc: 0.9988 \tTest Loss: 0.44625 \tTest Acc: 0.9238 \tTime: 68.3286\n",
      "Epoch: 441 \tTrain Loss: 0.00351 \tTrain Acc: 0.9989 \tTest Loss: 0.44620 \tTest Acc: 0.9238 \tTime: 68.2854\n",
      "Epoch: 442 \tTrain Loss: 0.00342 \tTrain Acc: 0.9988 \tTest Loss: 0.44547 \tTest Acc: 0.9246 \tTime: 68.3885\n",
      "Epoch: 443 \tTrain Loss: 0.00380 \tTrain Acc: 0.9988 \tTest Loss: 0.44509 \tTest Acc: 0.9239 \tTime: 67.5780\n",
      "Epoch: 444 \tTrain Loss: 0.00325 \tTrain Acc: 0.9989 \tTest Loss: 0.44679 \tTest Acc: 0.9236 \tTime: 67.5003\n",
      "Epoch: 445 \tTrain Loss: 0.00442 \tTrain Acc: 0.9987 \tTest Loss: 0.44464 \tTest Acc: 0.9248 \tTime: 67.5052\n",
      "Epoch: 446 \tTrain Loss: 0.00316 \tTrain Acc: 0.9990 \tTest Loss: 0.44392 \tTest Acc: 0.9244 \tTime: 67.4799\n",
      "Epoch: 447 \tTrain Loss: 0.00406 \tTrain Acc: 0.9987 \tTest Loss: 0.44359 \tTest Acc: 0.9259 \tTime: 67.2915\n",
      "Epoch: 448 \tTrain Loss: 0.00362 \tTrain Acc: 0.9988 \tTest Loss: 0.44523 \tTest Acc: 0.9249 \tTime: 67.4226\n",
      "Epoch: 449 \tTrain Loss: 0.00368 \tTrain Acc: 0.9990 \tTest Loss: 0.44564 \tTest Acc: 0.9251 \tTime: 67.4926\n",
      "Epoch: 450 \tTrain Loss: 0.00338 \tTrain Acc: 0.9989 \tTest Loss: 0.44575 \tTest Acc: 0.9238 \tTime: 67.5279\n",
      "Epoch: 451 \tTrain Loss: 0.00367 \tTrain Acc: 0.9988 \tTest Loss: 0.44420 \tTest Acc: 0.9239 \tTime: 67.4894\n",
      "Epoch: 452 \tTrain Loss: 0.00341 \tTrain Acc: 0.9990 \tTest Loss: 0.44559 \tTest Acc: 0.9247 \tTime: 67.4449\n",
      "Epoch: 453 \tTrain Loss: 0.00329 \tTrain Acc: 0.9988 \tTest Loss: 0.44618 \tTest Acc: 0.9248 \tTime: 67.3183\n",
      "Epoch: 454 \tTrain Loss: 0.00352 \tTrain Acc: 0.9988 \tTest Loss: 0.44627 \tTest Acc: 0.9251 \tTime: 67.4414\n",
      "Epoch: 455 \tTrain Loss: 0.00355 \tTrain Acc: 0.9989 \tTest Loss: 0.44630 \tTest Acc: 0.9250 \tTime: 67.6316\n",
      "Epoch: 456 \tTrain Loss: 0.00332 \tTrain Acc: 0.9989 \tTest Loss: 0.44564 \tTest Acc: 0.9243 \tTime: 67.4550\n",
      "Epoch: 457 \tTrain Loss: 0.00342 \tTrain Acc: 0.9988 \tTest Loss: 0.44686 \tTest Acc: 0.9240 \tTime: 67.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 458 \tTrain Loss: 0.00317 \tTrain Acc: 0.9989 \tTest Loss: 0.44508 \tTest Acc: 0.9245 \tTime: 67.5027\n",
      "Epoch: 459 \tTrain Loss: 0.00335 \tTrain Acc: 0.9989 \tTest Loss: 0.44644 \tTest Acc: 0.9254 \tTime: 67.4409\n",
      "Epoch: 460 \tTrain Loss: 0.00337 \tTrain Acc: 0.9990 \tTest Loss: 0.44523 \tTest Acc: 0.9249 \tTime: 67.4284\n",
      "Epoch: 461 \tTrain Loss: 0.00374 \tTrain Acc: 0.9987 \tTest Loss: 0.44504 \tTest Acc: 0.9253 \tTime: 67.3982\n",
      "Epoch: 462 \tTrain Loss: 0.00378 \tTrain Acc: 0.9989 \tTest Loss: 0.44498 \tTest Acc: 0.9260 \tTime: 67.4619\n",
      "Epoch: 463 \tTrain Loss: 0.00445 \tTrain Acc: 0.9986 \tTest Loss: 0.44533 \tTest Acc: 0.9260 \tTime: 67.4788\n",
      "Epoch: 464 \tTrain Loss: 0.00315 \tTrain Acc: 0.9989 \tTest Loss: 0.44669 \tTest Acc: 0.9254 \tTime: 67.4862\n",
      "Epoch: 465 \tTrain Loss: 0.00330 \tTrain Acc: 0.9989 \tTest Loss: 0.44467 \tTest Acc: 0.9249 \tTime: 67.3571\n",
      "Epoch: 466 \tTrain Loss: 0.00296 \tTrain Acc: 0.9991 \tTest Loss: 0.44516 \tTest Acc: 0.9257 \tTime: 67.3818\n",
      "Epoch: 467 \tTrain Loss: 0.00354 \tTrain Acc: 0.9989 \tTest Loss: 0.44628 \tTest Acc: 0.9260 \tTime: 67.4600\n",
      "Epoch: 468 \tTrain Loss: 0.00388 \tTrain Acc: 0.9988 \tTest Loss: 0.44557 \tTest Acc: 0.9255 \tTime: 67.4051\n",
      "Epoch: 469 \tTrain Loss: 0.00321 \tTrain Acc: 0.9989 \tTest Loss: 0.44507 \tTest Acc: 0.9252 \tTime: 67.3041\n",
      "Epoch: 470 \tTrain Loss: 0.00370 \tTrain Acc: 0.9989 \tTest Loss: 0.44506 \tTest Acc: 0.9250 \tTime: 67.3464\n",
      "Epoch: 471 \tTrain Loss: 0.00380 \tTrain Acc: 0.9988 \tTest Loss: 0.44562 \tTest Acc: 0.9254 \tTime: 67.4523\n",
      "Epoch: 472 \tTrain Loss: 0.00425 \tTrain Acc: 0.9986 \tTest Loss: 0.44492 \tTest Acc: 0.9254 \tTime: 67.4706\n",
      "Epoch: 473 \tTrain Loss: 0.00329 \tTrain Acc: 0.9990 \tTest Loss: 0.44552 \tTest Acc: 0.9253 \tTime: 67.3753\n",
      "Epoch: 474 \tTrain Loss: 0.00347 \tTrain Acc: 0.9990 \tTest Loss: 0.44509 \tTest Acc: 0.9257 \tTime: 66.5156\n",
      "Epoch: 475 \tTrain Loss: 0.00399 \tTrain Acc: 0.9987 \tTest Loss: 0.44280 \tTest Acc: 0.9264 \tTime: 62.9750\n",
      "Epoch: 476 \tTrain Loss: 0.00339 \tTrain Acc: 0.9988 \tTest Loss: 0.44457 \tTest Acc: 0.9248 \tTime: 62.9706\n",
      "Epoch: 477 \tTrain Loss: 0.00334 \tTrain Acc: 0.9988 \tTest Loss: 0.44541 \tTest Acc: 0.9248 \tTime: 63.0201\n",
      "Epoch: 478 \tTrain Loss: 0.00403 \tTrain Acc: 0.9987 \tTest Loss: 0.44237 \tTest Acc: 0.9260 \tTime: 62.9886\n",
      "Epoch: 479 \tTrain Loss: 0.00324 \tTrain Acc: 0.9991 \tTest Loss: 0.44347 \tTest Acc: 0.9256 \tTime: 62.9755\n",
      "Epoch: 480 \tTrain Loss: 0.00316 \tTrain Acc: 0.9990 \tTest Loss: 0.44429 \tTest Acc: 0.9258 \tTime: 62.9997\n",
      "Epoch: 481 \tTrain Loss: 0.00318 \tTrain Acc: 0.9989 \tTest Loss: 0.44599 \tTest Acc: 0.9255 \tTime: 63.0559\n",
      "Epoch: 482 \tTrain Loss: 0.00390 \tTrain Acc: 0.9987 \tTest Loss: 0.44771 \tTest Acc: 0.9261 \tTime: 63.0049\n",
      "Epoch: 483 \tTrain Loss: 0.00256 \tTrain Acc: 0.9992 \tTest Loss: 0.44752 \tTest Acc: 0.9267 \tTime: 62.9640\n",
      "Epoch: 484 \tTrain Loss: 0.00369 \tTrain Acc: 0.9990 \tTest Loss: 0.44707 \tTest Acc: 0.9258 \tTime: 62.9957\n",
      "Epoch: 485 \tTrain Loss: 0.00268 \tTrain Acc: 0.9992 \tTest Loss: 0.44776 \tTest Acc: 0.9256 \tTime: 64.0675\n",
      "Epoch: 486 \tTrain Loss: 0.00356 \tTrain Acc: 0.9988 \tTest Loss: 0.44721 \tTest Acc: 0.9255 \tTime: 64.4042\n",
      "Epoch: 487 \tTrain Loss: 0.00341 \tTrain Acc: 0.9990 \tTest Loss: 0.44708 \tTest Acc: 0.9257 \tTime: 64.4562\n",
      "Epoch: 488 \tTrain Loss: 0.00311 \tTrain Acc: 0.9990 \tTest Loss: 0.44532 \tTest Acc: 0.9255 \tTime: 64.4399\n",
      "Epoch: 489 \tTrain Loss: 0.00377 \tTrain Acc: 0.9987 \tTest Loss: 0.44477 \tTest Acc: 0.9259 \tTime: 64.4316\n",
      "Epoch: 490 \tTrain Loss: 0.00326 \tTrain Acc: 0.9988 \tTest Loss: 0.44393 \tTest Acc: 0.9258 \tTime: 64.5069\n",
      "Epoch: 491 \tTrain Loss: 0.00319 \tTrain Acc: 0.9988 \tTest Loss: 0.44664 \tTest Acc: 0.9257 \tTime: 64.4883\n",
      "Epoch: 492 \tTrain Loss: 0.00338 \tTrain Acc: 0.9987 \tTest Loss: 0.44664 \tTest Acc: 0.9250 \tTime: 64.4549\n",
      "Epoch: 493 \tTrain Loss: 0.00308 \tTrain Acc: 0.9991 \tTest Loss: 0.44653 \tTest Acc: 0.9252 \tTime: 64.4231\n",
      "Epoch: 494 \tTrain Loss: 0.00318 \tTrain Acc: 0.9990 \tTest Loss: 0.44536 \tTest Acc: 0.9254 \tTime: 64.4298\n",
      "Epoch: 495 \tTrain Loss: 0.00366 \tTrain Acc: 0.9989 \tTest Loss: 0.44570 \tTest Acc: 0.9258 \tTime: 64.4548\n",
      "Epoch: 496 \tTrain Loss: 0.00367 \tTrain Acc: 0.9989 \tTest Loss: 0.44559 \tTest Acc: 0.9260 \tTime: 64.4648\n",
      "Epoch: 497 \tTrain Loss: 0.00298 \tTrain Acc: 0.9990 \tTest Loss: 0.44516 \tTest Acc: 0.9253 \tTime: 64.4141\n",
      "Epoch: 498 \tTrain Loss: 0.00350 \tTrain Acc: 0.9989 \tTest Loss: 0.44559 \tTest Acc: 0.9250 \tTime: 64.4167\n",
      "Epoch: 499 \tTrain Loss: 0.00293 \tTrain Acc: 0.9992 \tTest Loss: 0.44492 \tTest Acc: 0.9252 \tTime: 64.4617\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(Nepoch,Nepoch+200): \n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    t1 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainDataLoader):\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = net(images)\n",
    "        fit = Loss(predicted_output,labels)\n",
    "        fit.backward()\n",
    "        optimizer.step()   \n",
    "        train_loss += fit.item()\n",
    "        train_acc += torch.sum(labels == predicted_output.argmax(dim=1)).item()\n",
    "\n",
    "    for i, data in enumerate(testDataLoader):\n",
    "        with torch.no_grad():\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            predicted_output = net(images)\n",
    "            fit = Loss(predicted_output,labels)\n",
    "            test_loss += fit.item()\n",
    "            test_acc += torch.sum(labels == predicted_output.argmax(dim=1)).item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = train_loss/len(trainDataLoader)\n",
    "    test_loss = test_loss/len(testDataLoader)\n",
    "    train_acc = train_acc/len(trainingdata)\n",
    "    test_acc = test_acc/len(testdata)\n",
    "    train_acc_history.append(train_acc)\n",
    "    test_acc_history.append(test_acc)\n",
    "    t2 = time.time()\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\tTrain Loss: {train_loss:.5f} \\tTrain Acc: {train_acc:.4f} \\tTest Loss: {test_loss:.5f} \\tTest Acc: {test_acc:.4f} \\tTime: {t2-t1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv2AZOtsX7eB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('Train_Acc_Run_9_Batch_64_fast.txt',train_acc_history)\n",
    "np.savetxt('Test_Acc_Run_9_Batch_64_fast.txt',test_acc_history)\n",
    "np.savetxt('Train_Loss_Run_9_Batch_64_fast.txt',train_loss_history)\n",
    "np.savetxt('Test_Loss_Run_9_Batch_64_fast.txt',test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTptu5T4X7eB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame (train_acc_history)\n",
    "filepath = 'Train_Acc_Run_9_Batch_64_fast.xlsx'\n",
    "df.to_excel(filepath, index=False)\n",
    "\n",
    "df = pd.DataFrame (test_acc_history)\n",
    "filepath = 'Test_Acc_Run_9_Batch_64_fast.xlsx'\n",
    "df.to_excel(filepath, index=False)\n",
    "\n",
    "\n",
    "df = pd.DataFrame (train_loss_history)\n",
    "filepath = 'Train_Loss_Run_9_Batch_64_fast.xlsx'\n",
    "df.to_excel(filepath, index=False)\n",
    "\n",
    "\n",
    "df = pd.DataFrame (test_loss_history)\n",
    "filepath = 'Test_Loss_Run_9_Batch_64_fast.xlsx'\n",
    "df.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgk7ioW0X7eC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = './project1_model_run_9_batch_64_fast.pt'\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQoqc29jX7eC",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Long_Run_Base_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0081205de10a45eb95ab0fa8cf5d1b54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9148add8cc434e6085a7e0534d3040b3",
      "placeholder": "​",
      "style": "IPY_MODEL_69d639e7861e40a2b3f41448d639c69c",
      "value": ""
     }
    },
    "169e435fd1af40afbb886aa420d0308b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "69d639e7861e40a2b3f41448d639c69c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7586a4d1d847421494ec6c24b7726da5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6c49e0105bd4cba86c7cfdc02bc68b8",
      "placeholder": "​",
      "style": "IPY_MODEL_8085b344ece049e1a9056e65dd8fc3a3",
      "value": " 170499072/? [00:04&lt;00:00, 37008722.06it/s]"
     }
    },
    "8085b344ece049e1a9056e65dd8fc3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9148add8cc434e6085a7e0534d3040b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91e133612d8c447897c0e9c1634c9aff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0081205de10a45eb95ab0fa8cf5d1b54",
       "IPY_MODEL_a1fd6468adf1485fbd287ffbd8486e72",
       "IPY_MODEL_7586a4d1d847421494ec6c24b7726da5"
      ],
      "layout": "IPY_MODEL_a078c1f3c09844b68e8147517c5cdd9b"
     }
    },
    "a078c1f3c09844b68e8147517c5cdd9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1fd6468adf1485fbd287ffbd8486e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db8b150cbf3f4104834164a15b7b95dd",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_169e435fd1af40afbb886aa420d0308b",
      "value": 170498071
     }
    },
    "d6c49e0105bd4cba86c7cfdc02bc68b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db8b150cbf3f4104834164a15b7b95dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
